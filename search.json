[
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression slope",
    "section": "",
    "text": "◎◉○"
  },
  {
    "objectID": "regression.html#live-simulation",
    "href": "regression.html#live-simulation",
    "title": "Regression slope",
    "section": "Live simulation",
    "text": "Live simulation\n\n\njstat = require(\"jstat@1.9.6\")\n\nclrs = ({\n  gold: \"#f3d567\",\n  orange: \"#ee9b43\",\n  coral: \"#e74b47\",\n  crimson: \"#b80422\",\n  navy: \"#172767\",\n  teal: \"#19798b\"\n})\n\n// Simple OLS helper: returns { intercept, slope }\nfunction ols(xvals, yvals) {\n  const n = xvals.length;\n  const mx = d3.mean(xvals);\n  const my = d3.mean(yvals);\n  let ssxy = 0, ssxx = 0;\n  for (let i = 0; i &lt; n; i++) {\n    ssxy += (xvals[i] - mx) * (yvals[i] - my);\n    ssxx += (xvals[i] - mx) * (xvals[i] - mx);\n  }\n  const slope = ssxy / ssxx;\n  const intercept = my - slope * mx;\n  return { intercept, slope };\n}\n\nfunction statLabel(value, textFn, dy, nullValues) {\n  const extent = d3.extent([...nullValues, value]);\n  const range = extent[1] - extent[0];\n  const pos = range === 0 ? 0.5 :\n    (value - extent[0]) / range;\n  const textAnchor = pos &gt; 0.82 ? \"end\" :\n    pos &lt; 0.18 ? \"start\" : \"middle\";\n  const dx = textAnchor === \"end\" ? -10 :\n    textAnchor === \"start\" ? 10 : 0;\n\n  const common = {\n    x: d =&gt; d, frameAnchor: \"top\", dy, dx,\n    text: textFn,\n    fontWeight: \"bold\", fontSize: 14,\n    textAnchor, paintOrder: \"stroke\"\n  };\n\n  return [\n    Plot.text([value], {\n      ...common,\n      stroke: \"white\", strokeWidth: 4, fill: \"black\"\n    })\n  ];\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof sample_size = Inputs.range([30, 2000], {\n  step: 10,\n  value: 100,\n  label: \"Sample size:\"\n})\n\nviewof slope_size = Inputs.range([-5, 5], {\n  step: 0.1,\n  value: 0.8,\n  label: \"Slope:\"\n})\n\nviewof spread = Inputs.range([1, 15], {\n  step: 1,\n  value: 5,\n  label: \"Noise (σ):\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_data = {\n  const data = [];\n  for (let i = 0; i &lt; sample_size; i++) {\n    const x = Math.random() * 10;\n    const y = 20 + slope_size * x +\n      jstat.normal.sample(0, spread);\n    data.push({ x, y });\n  }\n  return data;\n}\n\nobs_fit = ols(\n  sample_data.map(d =&gt; d.x),\n  sample_data.map(d =&gt; d.y)\n)\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  marginLeft: 50,\n  height: 280,\n  x: { label: \"Explanatory variable (x)\" },\n  y: { label: \"Outcome variable (y)\" },\n  marks: [\n    Plot.dot(sample_data, {\n      x: \"x\",\n      y: \"y\",\n      fill: clrs.teal,\n      fillOpacity: 0.6,\n      r: 3\n    }),\n    Plot.line(\n      [\n        { x: 0, y: obs_fit.intercept },\n        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }\n      ],\n      {\n        x: \"x\",\n        y: \"y\",\n        stroke: clrs.crimson,\n        strokeWidth: 2.5\n      }\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Calculate δStep 2: Simulate null worldStep 3: Put δ in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic (δ) is the regression slope—the estimated change in y for a one-unit increase in x.\n\nmodel_stats = {\n  const xvals = sample_data.map(d =&gt; d.x);\n  const yvals = sample_data.map(d =&gt; d.y);\n  const fit = ols(xvals, yvals);\n  const n = xvals.length;\n\n  // Residual standard error\n  const mx = d3.mean(xvals);\n  let ssxx = 0, sse = 0;\n  for (let i = 0; i &lt; n; i++) {\n    const pred = fit.intercept + fit.slope * xvals[i];\n    sse += (yvals[i] - pred) ** 2;\n    ssxx += (xvals[i] - mx) ** 2;\n  }\n  const rse = Math.sqrt(sse / (n - 2));\n  const se_slope = rse / Math.sqrt(ssxx);\n  const se_intercept = rse *\n    Math.sqrt(1 / n + mx * mx / ssxx);\n\n  return {\n    intercept: fit.intercept,\n    slope: fit.slope,\n    se_slope,\n    se_intercept,\n    ci_slope: [\n      fit.slope - 1.96 * se_slope,\n      fit.slope + 1.96 * se_slope\n    ],\n    ci_intercept: [\n      fit.intercept - 1.96 * se_intercept,\n      fit.intercept + 1.96 * se_intercept\n    ]\n  };\n}\n\nobs_stat = model_stats.slope\n\nobs_stat_abs = Math.abs(obs_stat).toFixed(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe regression slope is .\n\nhtml`&lt;table class=\"table table-sm\" style=\"max-width: 500px;\"&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Term&lt;/th&gt;\n      &lt;th&gt;Estimate&lt;/th&gt;\n      &lt;th&gt;95% CI&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;td&gt;(Intercept)&lt;/td&gt;\n      &lt;td&gt;${model_stats.intercept.toFixed(3)}&lt;/td&gt;\n      &lt;td&gt;[${model_stats.ci_intercept[0].toFixed(3)}, ${model_stats.ci_intercept[1].toFixed(3)}]&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=\"background-color: ${clrs.crimson}; color: white; font-weight: bold;\"&gt;\n      &lt;td&gt;x&lt;/td&gt;\n      &lt;td&gt;${model_stats.slope.toFixed(3)}&lt;/td&gt;\n      &lt;td&gt;[${model_stats.ci_slope[0].toFixed(3)}, ${model_stats.ci_slope[1].toFixed(3)}]&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;`\n\n\n\n\n\n\n\n\nWe create a null distribution by shuffling (or “permuting” to use the official stats term) the values of x. This simulates a world where all the real, measured values of both x and y are still the same, but where the relationship between x and y doesn’t matter. This eliminates any association between x and y.\nThink of this as being a world where there is no relationship between x and y. Importantly, this doesn’t mean that the slope is exactly 0. There is variation in the data, and that variation is reflected in the null world. What it means is that in the null world, the slope is 0 ± some amount.\nHere’s what one shuffle looks like. Notice that the y values stay the same—only the x values get reassigned:\n\nviewof reshuffle = Inputs.button(\"Reshuffle\")\n\n\n\n\n\n\n\nshuffle_preview = {\n  reshuffle;\n  const subset = sample_data.slice(0, 8);\n  const xvals = subset.map(d =&gt; d.x);\n  const shuffled_x = xvals.slice();\n  for (let i = shuffled_x.length - 1; i &gt; 0; i--) {\n    const j = Math.floor(Math.random() * (i + 1));\n    [shuffled_x[i], shuffled_x[j]] =\n      [shuffled_x[j], shuffled_x[i]];\n  }\n  return {\n    original: subset.map((d, i) =&gt; ({\n      \" \": i + 1,\n      x: +d.x.toFixed(1),\n      y: +d.y.toFixed(1)\n    })),\n    shuffled: subset.map((d, i) =&gt; ({\n      \" \": i + 1,\n      x: +shuffled_x[i].toFixed(1),\n      y: +d.y.toFixed(1)\n    }))\n  };\n}\n\n\n\n\n\n\n\n\nOriginal data\n\nInputs.table(shuffle_preview.original, {\n  columns: [\" \", \"x\", \"y\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nShuffled data\n\nInputs.table(shuffle_preview.shuffled, {\n  columns: [\" \", \"x\", \"y\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nWhen we do this shuffle hundreds of times and fit a regression each time, we get a null distribution—a picture of what slopes look like in a world where x and y are unrelated.\nHere’s what this null world looks like:\n\nviewof n_reps = Inputs.range([100, 2000], {\n  step: 100,\n  value: 500,\n  label: \"Number of simulations:\"\n})\n\n\n\n\n\n\n\n\nnull_results = {\n  const xvals = sample_data.map(d =&gt; d.x);\n  const yvals = sample_data.map(d =&gt; d.y);\n  const n = xvals.length;\n  const stats = [];\n  const lines = [];\n\n  for (let r = 0; r &lt; n_reps; r++) {\n    // Fisher-Yates shuffle of x values\n    const shuffled_x = xvals.slice();\n    for (let i = n - 1; i &gt; 0; i--) {\n      const j = Math.floor(Math.random() * (i + 1));\n      [shuffled_x[i], shuffled_x[j]] =\n        [shuffled_x[j], shuffled_x[i]];\n    }\n\n    const fit = ols(shuffled_x, yvals);\n    stats.push({ stat: fit.slope });\n    lines.push({\n      intercept: fit.intercept,\n      slope: fit.slope\n    });\n  }\n  return { stats, lines };\n}\n\nnull_dist = null_results.stats\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Slope\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s another way to see the null world slopes. Each thin line is a regression line for one of the simulated worlds where x and y are unrelated.\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: {\n    label: \"Explanatory variable (x)\",\n    domain: [0, 10]\n  },\n  y: { label: \"Outcome variable (y)\" },\n  marks: [\n    // Null world regression lines\n    ...null_results.lines.map(l =&gt;\n      Plot.line(\n        [\n          { x: 0, y: l.intercept },\n          { x: 10, y: l.intercept + l.slope * 10 }\n        ],\n        {\n          x: \"x\",\n          y: \"y\",\n          stroke: \"black\",\n          strokeWidth: 0.1,\n          strokeOpacity: 0.3\n        }\n      )\n    ),\n    Plot.dot(sample_data, {\n      x: \"x\",\n      y: \"y\",\n      fill: clrs.teal,\n      fillOpacity: 0.4,\n      r: 3\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\nNext we put δ inside that null world and see how comfortably it fits there.\nIs it surprising to see the red line in this null world? Is the line way out to one of the sides, or is it near the middle with the rest of the null world?\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Slope\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    ),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${d.toFixed(3)}`, 20,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\nOr alternatively, we can put the observed regression line from the actual data into the scatterplot with the null world regression lines. Is it surprising to see the red line in this null world?\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: {\n    label: \"Explanatory variable (x)\",\n    domain: [0, 10]\n  },\n  y: { label: \"Outcome variable (y)\" },\n  marks: [\n    ...null_results.lines.map(l =&gt;\n      Plot.line(\n        [\n          { x: 0, y: l.intercept },\n          { x: 10, y: l.intercept + l.slope * 10 }\n        ],\n        {\n          x: \"x\",\n          y: \"y\",\n          stroke: \"black\",\n          strokeWidth: 0.1,\n          strokeOpacity: 0.3\n        }\n      )\n    ),\n    Plot.dot(sample_data, {\n      x: \"x\",\n      y: \"y\",\n      fill: clrs.teal,\n      fillOpacity: 0.4,\n      r: 3\n    }),\n    Plot.line(\n      [\n        { x: 0, y: obs_fit.intercept },\n        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }\n      ],\n      {\n        x: \"x\",\n        y: \"y\",\n        stroke: \"red\",\n        strokeWidth: 3\n      }\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\nWe can actually quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a δ at least that extreme in a world where there’s no relationship between x and y.\n\n\np_value = {\n  const abs_obs = Math.abs(obs_stat);\n  const extreme = null_dist.filter(\n    d =&gt; Math.abs(d.stat) &gt;= abs_obs\n  ).length;\n  return extreme / null_dist.length;\n}\n\np_value_clean = p_value === 0\n  ? \"&lt; 0.001\"\n  : p_value.toFixed(3)\n\np_percent = p_value === 0\n  ? \"&lt; 0.1%\"\n  : (p_value * 100).toFixed(1) + \"%\"\n\nnull_bins = {\n  const values = null_dist.map(d =&gt; d.stat);\n  const bin = d3.bin().thresholds(30);\n  const bins = bin(values);\n  const abs_obs = Math.abs(obs_stat);\n  return bins.map(b =&gt; ({\n    x0: b.x0,\n    x1: b.x1,\n    count: b.length,\n    extreme: Math.abs((b.x0 + b.x1) / 2) &gt;= abs_obs\n  }));\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Slope\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${d.toFixed(3)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Also show the slope plot with observed line\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: {\n    label: \"Explanatory variable (x)\",\n    domain: [0, 10]\n  },\n  y: { label: \"Outcome variable (y)\" },\n  marks: [\n    ...null_results.lines.map(l =&gt;\n      Plot.line(\n        [\n          { x: 0, y: l.intercept },\n          { x: 10, y: l.intercept + l.slope * 10 }\n        ],\n        {\n          x: \"x\",\n          y: \"y\",\n          stroke: \"black\",\n          strokeWidth: 0.1,\n          strokeOpacity: 0.3\n        }\n      )\n    ),\n    Plot.dot(sample_data, {\n      x: \"x\",\n      y: \"y\",\n      fill: clrs.teal,\n      fillOpacity: 0.4,\n      r: 3\n    }),\n    Plot.line(\n      [\n        { x: 0, y: obs_fit.intercept },\n        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }\n      ],\n      {\n        x: \"x\",\n        y: \"y\",\n        stroke: \"red\",\n        strokeWidth: 3\n      }\n    )\n  ]\n})\n\n\n\n\n\n\n\nThe p-value is \n\nhtml`&lt;p&gt;This means that in a world where there is no relationship between x and y, there is a &lt;strong&gt;${p_percent}&lt;/strong&gt; chance of seeing a slope of at least &lt;strong&gt;${obs_stat_abs}&lt;/strong&gt;&lt;/p&gt;`\n\n\n\n\n\n\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nThere are lots of possible thresholds. By convention, most people use a threshold (often shortened to α) of 0.05, or 5%. But that’s not required! You could have a lower standard with an α of 0.1 (10%), or a higher standard with an α of 0.01 (1%).\n\nviewof alpha = Inputs.select([0.10, 0.05, 0.01], {\n  label: \"Significance threshold (α):\",\n  value: 0.05\n})\n\n\n\n\n\n\n\n\n\n{\n  if (p_value &lt; alpha) {\n    return html`&lt;div class=\"alert alert-success\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where there is no relationship between x and y, the probability of seeing a slope of at least &lt;strong&gt;${obs_stat_abs}&lt;/strong&gt; is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is less than ${alpha}, we have enough evidence to say that the slope is &lt;strong&gt;statistically significant.&lt;/strong&gt;&lt;/p&gt;\n    &lt;/div&gt;`;\n  } else {\n    return html`&lt;div class=\"alert alert-warning\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Not statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where there is no relationship between x and y, the probability of seeing a slope of at least &lt;strong&gt;${obs_stat_abs}&lt;/strong&gt; is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is greater than ${alpha}, we don't have enough evidence to say that the slope doesn't come from the null world. The slope is thus &lt;strong&gt;not statistically significant.&lt;/strong&gt;&lt;/p&gt;\n      &lt;hr&gt;\n      &lt;p style=\"font-size: 0.9em;\"&gt;This does &lt;strong&gt;not&lt;/strong&gt; mean that there is no relationship between x and y! We just don't have enough evidence to judge if there's a relationship.&lt;/p&gt;\n    &lt;/div&gt;`;\n  }\n}\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  x: { label: \"Slope\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${d.toFixed(3)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  x: {\n    label: \"Explanatory variable (x)\",\n    domain: [0, 10]\n  },\n  y: { label: \"Outcome variable (y)\" },\n  marks: [\n    ...null_results.lines.map(l =&gt;\n      Plot.line(\n        [\n          { x: 0, y: l.intercept },\n          { x: 10, y: l.intercept + l.slope * 10 }\n        ],\n        {\n          x: \"x\",\n          y: \"y\",\n          stroke: \"black\",\n          strokeWidth: 0.1,\n          strokeOpacity: 0.3\n        }\n      )\n    ),\n    Plot.dot(sample_data, {\n      x: \"x\",\n      y: \"y\",\n      fill: clrs.teal,\n      fillOpacity: 0.4,\n      r: 3\n    }),\n    Plot.line(\n      [\n        { x: 0, y: obs_fit.intercept },\n        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }\n      ],\n      {\n        x: \"x\",\n        y: \"y\",\n        stroke: \"red\",\n        strokeWidth: 3\n      }\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\nEvidentiary standards\nWhen thinking about p-values and thresholds, I like to imagine myself as a judge or a member of a jury. Many legal systems around the world have formal evidentiary thresholds or standards of proof. If prosecutors provide evidence that meets a threshold (i.e. goes beyond a reasonable doubt, or shows evidence on a balance of probabilities), the judge or jury can rule guilty. If there’s not enough evidence to clear the standard or threshold, the judge or jury has to rule not guilty.\nWith p-values:\n\nIf the probability of seeing an effect or difference (or δ) in a null world is less than 5% (or whatever the threshold is), we rule it statistically significant and say that the difference does not fit in that world. We’re pretty confident that it’s not zero.\nIf the p-value is larger than the threshold, we do not have enough evidence to claim that δ doesn’t come from a world of where there’s no difference. We don’t know if it’s not zero.\n\nImportantly, if the difference is not significant, that does not mean that there is no difference. It just means that we can’t detect one if there is. If a prosecutor doesn’t provide sufficient evidence to clear a standard or threshold, it does not mean that the defendant didn’t do whatever they’re charged with†—it means that the judge or jury can’t detect guilt.\n\n\n\n\n\n\nNoteDifferent evidentiary standards\n\n\n\nMany legal systems have different levels of evidentiary standards:\n\nStandards of proof in most common law systems (juries):  \n\nBalance of probabilities (civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nEvidentiary thresholds in the United States (juries): \n\nPreponderance of the evidence (civil cases)\nClear and convincing evidence (more important civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nStandards of proof in China (judges):   \n\n高度盖然性 [gāo dù gài rán xìng] / highly probable (civil cases)\n证据确实充分 [zhèng jù què shí chōng fēn] / facts being clear and evidence being sufficient | the evidence is definite and sufficient (criminal cases)\n\nLevels of doubt in Sharia systems (judges):     \n\nغلبة الظن [ghalabat al-zann] / preponderance of assumption (ta’zir cases and family matters)\nاليقين [yaqin] / certainty (hudud/qisas cases)\n\nStandard of proof in the International Criminal Court (judges): \n\nBeyond reasonable doubt (genocide, crimes against humanity, or war crimes)"
  },
  {
    "objectID": "regression.html#flipper-length-and-body-mass",
    "href": "regression.html#flipper-length-and-body-mass",
    "title": "Regression slope",
    "section": "Flipper length and body mass",
    "text": "Flipper length and body mass\nFor this example, we want to know if flipper length predicts body mass in penguins near Palmer Station, Antarctica. Here’s a scatterplot of the relationship:\n\n\n\n\n\n\n\n\n\nThere’s a clear positive trend—penguins with longer flippers tend to be heavier. But is that relationship real, or could it just be due to random chance? Time for hypothesis testing!\nFirst, we’ll load some packages:\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(parameters)\n\npenguins &lt;- penguins |&gt; drop_na(sex)"
  },
  {
    "objectID": "regression.html#null-hypothesis-inference-with-infer",
    "href": "regression.html#null-hypothesis-inference-with-infer",
    "title": "Regression slope",
    "section": "Null hypothesis inference with {infer}",
    "text": "Null hypothesis inference with {infer}\n\nStep 1: Calculate \\(\\delta\\)Step 2: Simulate null worldStep 3: Put \\(\\delta\\) in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic we’re interested in is the regression slope—the estimated change in body mass for a one-unit (1 mm) increase in flipper length.\n\ndelta &lt;- penguins |&gt;\n  specify(body_mass ~ flipper_len) |&gt;\n  calculate(stat = \"slope\")\ndelta\n\nResponse: body_mass (numeric)\nExplanatory: flipper_len (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  50.2\n\n\nThe slope is 50.15, meaning that for every additional millimeter of flipper length, body mass increases by about 50.2 grams.\n\n\nWe create a null distribution by shuffling (or “permuting”) the flipper length values. This simulates a world where all the real, measured values of both flipper length and body mass are still the same, but where the relationship between them doesn’t matter. This eliminates any association between flipper length and body mass.\n\nshuffled_data &lt;- penguins |&gt;\n  specify(body_mass ~ flipper_len) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 5000, type = \"permute\")\n\nNext we fit a regression in each of these 5,000 shuffled worlds and extract the slope:\n\nnull_world &lt;- shuffled_data |&gt;\n  calculate(stat = \"slope\")\nnull_world\n\nResponse: body_mass (numeric)\nExplanatory: flipper_len (numeric)\nNull Hypothesis: independence\n# A tibble: 5,000 × 2\n   replicate   stat\n       &lt;int&gt;  &lt;dbl&gt;\n 1         1  2.27 \n 2         2  1.68 \n 3         3  4.57 \n 4         4  0.444\n 5         5 -0.249\n 6         6  3.07 \n 7         7  2.17 \n 8         8 -2.15 \n 9         9 -3.02 \n10        10 -2.78 \n# ℹ 4,990 more rows\n\n\nHere’s what this null world looks like:\n\nnull_world |&gt;\n  visualize()\n\n\n\n\n\n\n\n\nNotice that the slopes are centered around 0, reflecting a world where flipper length and body mass are unrelated.\n\n\nNext we put δ inside that null world to see how comfortably it fits there.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = NULL)\n\n\n\n\n\n\n\n\nThat’s way far to the right and doesn’t look likely at all. A slope of 50.15 is really unlikely in a world where flipper length and body mass are unrelated.\n\n\nWe can quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a slope at least that extreme in a world where there’s no relationship between flipper length and body mass.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")\n\n\n\n\n\n\n\n\n\np_value &lt;- null_world |&gt;\n  get_p_value(obs_stat = delta, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nThe p-value is &lt; 0.001. This means that in a world where flipper length has no relationship to body mass, there is a &lt; 0.1% chance of seeing a slope at least as extreme as 50.15.\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nUsing an α of 0.05, the p-value is &lt; 0.001, which is less than 0.05. We have enough evidence to say that the relationship between flipper length and body mass is statistically significant.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")"
  },
  {
    "objectID": "regression.html#null-hypothesis-inference-with-lm",
    "href": "regression.html#null-hypothesis-inference-with-lm",
    "title": "Regression slope",
    "section": "Null hypothesis inference with lm()",
    "text": "Null hypothesis inference with lm()\nIn practice, most people do not simulate null worlds. Instead, they fit a regression model with lm(), which uses a t-distribution to approximate the null world mathematically and test whether each coefficient is different from 0. The intuition is the same: a p-value is still the probability of seeing a slope at least that extreme in a world where the true slope is 0.\n\nmodel &lt;- lm(body_mass ~ flipper_len, data = penguins)\nsummary(model)\n\n\nCall:\nlm(formula = body_mass ~ flipper_len, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1057.33  -259.79   -12.24   242.97  1293.89 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5872.09     310.29  -18.93   &lt;2e-16 ***\nflipper_len    50.15       1.54   32.56   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 393.3 on 331 degrees of freedom\nMultiple R-squared:  0.7621,    Adjusted R-squared:  0.7614 \nF-statistic:  1060 on 1 and 331 DF,  p-value: &lt; 2.2e-16\n\n\nBuried in that output is the p-value for the flipper_len coefficient: p &lt; 2.2e-16, or p &lt; 2.2 × 10−16. That’s really tiny. In a world where flipper length had no relationship with body mass, it would be virtually impossible to see a slope as extreme as 50.15. We have enough evidence to declare that the relationship is statistically significant.\nIf you don’t like all that text output, you can feed the model to the model_parameters() function from the {parameters} package:\n\nmodel |&gt;\n  model_parameters() |&gt;\n  display(caption = \"\")\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                Parameter\n                Coefficient\n                SE\n                95% CI\n                t(331)\n                p\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -5872.09\n                  310.29\n                  (-6482.47, -5261.71)\n                  -18.92\n                  &lt; .001\n                \n                \n                  flipper len\n                  50.15\n                  1.54\n                  (47.12, 53.18)\n                  32.56\n                  &lt; .001"
  },
  {
    "objectID": "regression.html#footnotes",
    "href": "regression.html#footnotes",
    "title": "Regression slope",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKind of—in common law systems, defendants are presumed innocent until\nproven guilty, so if there’s not enough evidence to prove guilt, they\nare innocent by definition. ↩︎"
  },
  {
    "objectID": "index.html#there-is-only-one-test",
    "href": "index.html#there-is-only-one-test",
    "title": "Statistical testing in null worlds",
    "section": "There is only one test",
    "text": "There is only one test\nAt their core, all statistical tests† can be conducted by following a universal pattern:\n\nStep 1: Calculate a sample statistic, or \\(\\delta\\) (“delta”). This is the main measure you care about: the difference in means, the average, the median, the proportion, the difference in proportions, the slope in a regression model, an odds ratio, etc.\nStep 2: Use simulation to invent a world where \\(\\delta\\) is null. Simulate what the world would look like if there was no difference or relationship between two groups, or if there was no difference in proportions, or where the average value is a specific number.\nStep 3: Look at \\(\\delta\\) in the null world. Put the sample statistic in the null world and see if it fits well.\nStep 4: Calculate the probability that \\(\\delta\\) could exist in the null world. This is the p-value, or the probability that you’d see a \\(\\delta\\) at least as extreme in a world where there’s no difference.\nStep 5: Decide if \\(\\delta\\) is statistically significant. Choose some evidentiary standard or threshold for deciding if there’s sufficient proof for rejecting the null world. Standard thresholds (from least to most rigorous) are 0.1, 0.05, and 0.01.\n\nThat’s all. Five steps. No need to follow complicated flowcharts to select the best and most appropriate statistical test. No need to decide which pretests you need to run to choose the right flavor of t-test. No need to think about whether you should use a t- or a z- distribution for your test statistic.\nSimulate instead. There is only one test.\n\n\n\n\n\n\nTiptl;dr\n\n\n\nCalculate a number, simulate a null world, calculate the probability of seeing your number in that null world, and decide if that number is significantly different from what is typically seen in the null world."
  },
  {
    "objectID": "index.html#examples",
    "href": "index.html#examples",
    "title": "Statistical testing in null worlds",
    "section": "Examples",
    "text": "Examples\nThis site contains a few different illustrations of common statistical tests:\n\nDifference in means\nDifference in proportions\nOne-sample mean\nRegression slope"
  },
  {
    "objectID": "index.html#do-i-have-to-simulate-everything",
    "href": "index.html#do-i-have-to-simulate-everything",
    "title": "Statistical testing in null worlds",
    "section": "Do I have to simulate everything?",
    "text": "Do I have to simulate everything?\nNo! In practice, you’ll typically run regular regressions, t-tests, or \\(\\chi^2\\) tests. These use known theoretical distributions to approximate the null world mathematically rather than through simulation, but the intuition is identical: a p-value is still the probability of seeing a \\(\\delta\\) at least as extreme in a world where there’s no difference.\nOnce you understand this general framework, “you can understand any hypothesis test”, even if it’s not done through simulation.\nBut simulation really shines when your \\(\\delta\\) is something unusual—something that doesn’t have a convenient built-in test or a place in a statistical test flowchart.‡ Want to test a difference in group medians? Some interesting quantity that no textbook has ever named, like the average number of years a country ranks in the top five for GDP growth? You can test it! Define any \\(\\delta\\) you want, simulate a null world for it, and get a valid p-value. No flowchart required. The {infer} package in R makes this straightforward.\nFor more examples of this process, and a more formal explanation of why it works, see Allen Downey’s “There is still only one test” and chapters 7–10 of ModernDive (especially chapter 8)."
  },
  {
    "objectID": "index.html#this-all-feels-vaguely-bayesian",
    "href": "index.html#this-all-feels-vaguely-bayesian",
    "title": "Statistical testing in null worlds",
    "section": "This all feels vaguely Bayesian?",
    "text": "This all feels vaguely Bayesian?\nYep. All this simulation thinking is actually part of my clandestine plot to get more people curious about Bayesian statistics. This way of thinking prepares you for Bayesian inference, without actually being Bayesian.\nThis simulation-based approach mirrors a lot of the computational thinking behind Bayesian statistics. In both approaches, we (1) start with an explicit data-generating process and (2) think about uncertainty with simulation instead of formulas.\nBut there’s an important difference in what we’re simulating!\nWith traditional null hypothesis testing, we simulate a null world and ask:\n\nIf this were the world we lived in, how surprising would our observed \\(\\delta\\) be?\n\nBayesian statistics flips this. Instead of asking how extreme our data (or \\(\\delta\\)) is in a single null world, we model uncertainty about the parameter \\(\\delta\\) itself. After combining prior beliefs with observed data, we get a posterior distribution that we can simulate from to ask:\n\nWhat range of values of \\(\\delta\\) are most plausible, given the data we’ve observed?\n\nIf that sounds fun—and if you’d rather say things like “there’s an X% probability that \\(\\delta\\) is positive” or “\\(\\delta\\) is most likely between X and Y” instead of the convoluted “in a world where \\(\\delta\\) is 0ish, there’s an X% probability of seeing a \\(\\delta\\) at least as extreme as what we observed”—check out Bayes Rules! for the best introduction I’ve found to Bayesian modeling."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Statistical testing in null worlds",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt least for null hypothesis significance testing (NHST). Bayesian\ninference is different (see more about that below!). ↩︎\nSee chapter 1 of Richard McElreath’s Statistical Rethinking\nfor more about issues with statistical test flowcharts. He has a free lecture\nvideo and slides\nand other\nresources here. ↩︎"
  },
  {
    "objectID": "diff-in-props.html",
    "href": "diff-in-props.html",
    "title": "Difference in proportions",
    "section": "",
    "text": "◎◉○"
  },
  {
    "objectID": "diff-in-props.html#live-simulation",
    "href": "diff-in-props.html#live-simulation",
    "title": "Difference in proportions",
    "section": "Live simulation",
    "text": "Live simulation\n\n\njstat = require(\"jstat@1.9.6\")\n\nclrs = ({\n  gold: \"#f3d567\",\n  orange: \"#ee9b43\",\n  coral: \"#e74b47\",\n  crimson: \"#b80422\",\n  navy: \"#172767\",\n  teal: \"#19798b\"\n})\n\nfunction fmt_pp(x) {\n  return (x * 100).toFixed(1) + \" pp.\";\n}\n\nfunction fmt_pp0(x) {\n  return Math.round(x * 100) + \" pp.\";\n}\n\nfunction fmt_pct(x) {\n  return (x * 100).toFixed(1) + \"%\";\n}\n\nfunction statLabel(value, textFn, dy, nullValues) {\n  const extent = d3.extent([...nullValues, value]);\n  const range = extent[1] - extent[0];\n  const pos = range === 0 ? 0.5 :\n    (value - extent[0]) / range;\n  const textAnchor = pos &gt; 0.82 ? \"end\" :\n    pos &lt; 0.18 ? \"start\" : \"middle\";\n  const dx = textAnchor === \"end\" ? -10 :\n    textAnchor === \"start\" ? 10 : 0;\n\n  const common = {\n    x: d =&gt; d, frameAnchor: \"top\", dy, dx,\n    text: textFn,\n    fontWeight: \"bold\", fontSize: 14,\n    textAnchor, paintOrder: \"stroke\"\n  };\n\n  return [\n    Plot.text([value], {\n      ...common,\n      stroke: \"white\", strokeWidth: 4, fill: \"black\"\n    })\n  ];\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof sample_size = Inputs.range([30, 2000], {\n  step: 10,\n  value: 100,\n  label: \"Sample size:\"\n})\n\nviewof effect_size_pp = Inputs.range([-50, 50], {\n  step: 1,\n  value: 20,\n  label: \"Percentage-point difference:\"\n})\n\neffect_size = effect_size_pp / 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_data = {\n  const data = [];\n  for (let i = 0; i &lt; sample_size; i++) {\n    const group = Math.random() &lt; 0.5 ? \"A\" : \"B\";\n    const prob = group === \"A\" ? 0.5 : 0.5 + effect_size;\n    const outcome = Math.random() &lt; prob ? \"Agree\" : \"Disagree\";\n    data.push({ group, outcome });\n  }\n  return data;\n}\n\n// Compute proportions for stacked bar chart\nprop_data = {\n  const counts = {};\n  for (const d of sample_data) {\n    const key = d.group + \"|\" + d.outcome;\n    counts[key] = (counts[key] || 0) + 1;\n  }\n  const groupTotals = {};\n  for (const d of sample_data) {\n    groupTotals[d.group] = (groupTotals[d.group] || 0) + 1;\n  }\n  const result = [];\n  for (const [key, count] of Object.entries(counts)) {\n    const [group, outcome] = key.split(\"|\");\n    result.push({\n      group,\n      outcome,\n      prop: count / groupTotals[group]\n    });\n  }\n  return result;\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  marginLeft: 50,\n  height: 200,\n  x: { label: \"Proportion\", domain: [0, 1] },\n  y: { label: null, domain: [\"B\", \"A\"] },\n  color: {\n    domain: [\"Agree\", \"Disagree\"],\n    range: [clrs.navy, clrs.orange],\n    legend: true\n  },\n  marks: [\n    Plot.barX(prop_data, {\n      x: \"prop\",\n      y: \"group\",\n      fill: \"outcome\",\n      order: \"outcome\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Calculate δStep 2: Simulate null worldStep 3: Put δ in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic (δ) is the difference in proportions of how many people responded “Agree” between the two groups.\n\ngroup_stats = {\n  const groups = { A: { n: 0, agree: 0 }, B: { n: 0, agree: 0 } };\n  for (const d of sample_data) {\n    groups[d.group].n++;\n    if (d.outcome === \"Agree\") groups[d.group].agree++;\n  }\n\n  const propA = groups.A.agree / groups.A.n;\n  const propB = groups.B.agree / groups.B.n;\n  const nA = groups.A.n;\n  const nB = groups.B.n;\n\n  const seA = Math.sqrt(propA * (1 - propA) / nA);\n  const seB = Math.sqrt(propB * (1 - propB) / nB);\n  const ciA = [propA - 1.96 * seA, propA + 1.96 * seA];\n  const ciB = [propB - 1.96 * seB, propB + 1.96 * seB];\n\n  const diff = propB - propA;\n  const seDiff = Math.sqrt(seA * seA + seB * seB);\n  const ciDiff = [diff - 1.96 * seDiff, diff + 1.96 * seDiff];\n\n  return {\n    propA, propB, nA, nB, diff,\n    ciA, ciB, ciDiff\n  };\n}\n\nobs_stat = group_stats.diff\n\nobs_stat_pp = fmt_pp(Math.abs(obs_stat))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference in proportions is \n\nhtml`&lt;table class=\"table table-sm\" style=\"max-width: 500px;\"&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Group&lt;/th&gt;\n      &lt;th&gt;N&lt;/th&gt;\n      &lt;th&gt;Proportion&lt;/th&gt;\n      &lt;th&gt;95% CI&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr style=\"background-color: ${clrs.navy}11;\"&gt;\n      &lt;td&gt;A&lt;/td&gt;\n      &lt;td&gt;${group_stats.nA}&lt;/td&gt;\n      &lt;td&gt;${fmt_pct(group_stats.propA)}&lt;/td&gt;\n      &lt;td&gt;[${fmt_pct(group_stats.ciA[0])}, ${fmt_pct(group_stats.ciA[1])}]&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=\"background-color: ${clrs.orange}22;\"&gt;\n      &lt;td&gt;B&lt;/td&gt;\n      &lt;td&gt;${group_stats.nB}&lt;/td&gt;\n      &lt;td&gt;${fmt_pct(group_stats.propB)}&lt;/td&gt;\n      &lt;td&gt;[${fmt_pct(group_stats.ciB[0])}, ${fmt_pct(group_stats.ciB[1])}]&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=\"background-color: ${clrs.crimson}; color: white; font-weight: bold;\"&gt;\n      &lt;td&gt;Difference between % agree (B − A)&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n      &lt;td&gt;${fmt_pp(group_stats.diff)}&lt;/td&gt;\n      &lt;td&gt;[${fmt_pp(group_stats.ciDiff[0])}, ${fmt_pp(group_stats.ciDiff[1])}]&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;`\n\n\n\n\n\n\n\n\nWe create a null distribution by shuffling (or “permuting” to use the official stats term) the group labels. This simulates a world where all the real, measured responses are still the same, but where group assignment doesn’t matter. This eliminates all differences between the groups.\nThink of this as being a world where there are no differences between the two groups. Importantly, this doesn’t mean that the measured difference between the groups is exactly 0. There is variation in the data, and that variation is reflected in the null world. What it means is that in the null world, the difference between the two groups is 0 ± some amount.\nHere’s what one shuffle looks like. Notice that the responses stay the same—only the group labels get reassigned:\n\nviewof reshuffle = Inputs.button(\"Reshuffle\")\n\n\n\n\n\n\n\nshuffle_preview = {\n  reshuffle;\n  const subset = sample_data.slice(0, 8);\n  const groups = subset.map(d =&gt; d.group);\n  const shuffled = groups.slice();\n  for (let i = shuffled.length - 1; i &gt; 0; i--) {\n    const j = Math.floor(Math.random() * (i + 1));\n    [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];\n  }\n  return {\n    original: subset.map((d, i) =&gt; ({\n      \" \": i + 1,\n      Group: d.group,\n      Response: d.outcome\n    })),\n    shuffled: subset.map((d, i) =&gt; ({\n      \" \": i + 1,\n      Group: shuffled[i],\n      Response: d.outcome\n    }))\n  };\n}\n\n\n\n\n\n\n\n\nOriginal data\n\nInputs.table(shuffle_preview.original, {\n  columns: [\" \", \"Group\", \"Response\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nShuffled data\n\nInputs.table(shuffle_preview.shuffled, {\n  columns: [\" \", \"Group\", \"Response\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nWhen we do this shuffle hundreds of times and compute the difference in proportions each time, we get a null distribution—a picture of what differences look like in a world where groups don’t matter.\nHere’s what this null world looks like:\n\nviewof n_reps = Inputs.range([100, 2000], {\n  step: 100,\n  value: 500,\n  label: \"Number of simulations:\"\n})\n\n\n\n\n\n\n\n\nnull_dist = {\n  const outcomes = sample_data.map(d =&gt; d.outcome);\n  const groups = sample_data.map(d =&gt; d.group);\n  const n = outcomes.length;\n  const results = [];\n\n  for (let r = 0; r &lt; n_reps; r++) {\n    const shuffled = groups.slice();\n    for (let i = n - 1; i &gt; 0; i--) {\n      const j = Math.floor(Math.random() * (i + 1));\n      [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];\n    }\n\n    let agreeA = 0, nA = 0, agreeB = 0, nB = 0;\n    for (let i = 0; i &lt; n; i++) {\n      if (shuffled[i] === \"B\") {\n        nB++;\n        if (outcomes[i] === \"Agree\") agreeB++;\n      } else {\n        nA++;\n        if (outcomes[i] === \"Agree\") agreeA++;\n      }\n    }\n    results.push({ stat: agreeB / nB - agreeA / nA });\n  }\n  return results;\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: {\n    label: \"Difference between % agree (B − A)\",\n    tickFormat: d =&gt; fmt_pp0(d)\n  },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext we put δ inside that null world and see how comfortably it fits there.\nIs it surprising to see the red line in this null world? Is the line way out to one of the sides, or is it near the middle with the rest of the null world?\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: {\n    label: \"Difference between % agree (B − A)\",\n    tickFormat: d =&gt; fmt_pp0(d)\n  },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    ),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${fmt_pp(d)}`, 20,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\nWe can actually quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a δ at least that extreme in a world where there’s no difference between the group proportions.\n\n\np_value = {\n  const abs_obs = Math.abs(obs_stat);\n  const extreme = null_dist.filter(\n    d =&gt; Math.abs(d.stat) &gt;= abs_obs\n  ).length;\n  return extreme / null_dist.length;\n}\n\np_value_clean = p_value === 0\n  ? \"&lt; 0.001\"\n  : p_value.toFixed(3)\n\np_percent = p_value === 0\n  ? \"&lt; 0.1%\"\n  : (p_value * 100).toFixed(1) + \"%\"\n\nnull_bins = {\n  const values = null_dist.map(d =&gt; d.stat);\n  const bin = d3.bin().thresholds(30);\n  const bins = bin(values);\n  const abs_obs = Math.abs(obs_stat);\n  return bins.map(b =&gt; ({\n    x0: b.x0,\n    x1: b.x1,\n    count: b.length,\n    extreme: Math.abs((b.x0 + b.x1) / 2) &gt;= abs_obs\n  }));\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: {\n    label: \"Difference between % agree (B − A)\",\n    tickFormat: d =&gt; fmt_pp0(d)\n  },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${fmt_pp(d)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe p-value is \n\nhtml`&lt;p&gt;This means that in a world where there is no difference between the groups, there is a &lt;strong&gt;${p_percent}&lt;/strong&gt; chance of seeing a difference of at least &lt;strong&gt;${obs_stat_pp}&lt;/strong&gt;&lt;/p&gt;`\n\n\n\n\n\n\n\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nThere are lots of possible thresholds. By convention, most people use a threshold (often shortened to α) of 0.05, or 5%. But that’s not required! You could have a lower standard with an α of 0.1 (10%), or a higher standard with an α of 0.01 (1%).\n\nviewof alpha = Inputs.select([0.10, 0.05, 0.01], {\n  label: \"Significance threshold (α):\",\n  value: 0.05\n})\n\n\n\n\n\n\n\n\n\n{\n  if (p_value &lt; alpha) {\n    return html`&lt;div class=\"alert alert-success\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where there is no difference between the groups, the probability of seeing a difference of at least &lt;strong&gt;${obs_stat_pp}&lt;/strong&gt; is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is less than ${alpha}, we have enough evidence to say that the difference is &lt;strong&gt;statistically significant.&lt;/strong&gt;&lt;/p&gt;\n    &lt;/div&gt;`;\n  } else {\n    return html`&lt;div class=\"alert alert-warning\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Not statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where there is no difference between the groups, the probability of seeing a difference of at least &lt;strong&gt;${obs_stat_pp}&lt;/strong&gt; is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is greater than ${alpha}, we don't have enough evidence to say that the difference doesn't come from the null world. The difference is thus &lt;strong&gt;not statistically significant.&lt;/strong&gt;&lt;/p&gt;\n      &lt;hr&gt;\n      &lt;p style=\"font-size: 0.9em;\"&gt;This does &lt;strong&gt;not&lt;/strong&gt; mean that there is no difference between the groups! We just don't have enough evidence to judge if there's a difference.&lt;/p&gt;\n    &lt;/div&gt;`;\n  }\n}\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  x: {\n    label: \"Difference between % agree (B − A)\",\n    tickFormat: d =&gt; fmt_pp0(d)\n  },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${fmt_pp(d)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\nEvidentiary standards\nWhen thinking about p-values and thresholds, I like to imagine myself as a judge or a member of a jury. Many legal systems around the world have formal evidentiary thresholds or standards of proof. If prosecutors provide evidence that meets a threshold (i.e. goes beyond a reasonable doubt, or shows evidence on a balance of probabilities), the judge or jury can rule guilty. If there’s not enough evidence to clear the standard or threshold, the judge or jury has to rule not guilty.\nWith p-values:\n\nIf the probability of seeing an effect or difference (or δ) in a null world is less than 5% (or whatever the threshold is), we rule it statistically significant and say that the difference does not fit in that world. We’re pretty confident that it’s not zero.\nIf the p-value is larger than the threshold, we do not have enough evidence to claim that δ doesn’t come from a world of where there’s no difference. We don’t know if it’s not zero.\n\nImportantly, if the difference is not significant, that does not mean that there is no difference. It just means that we can’t detect one if there is. If a prosecutor doesn’t provide sufficient evidence to clear a standard or threshold, it does not mean that the defendant didn’t do whatever they’re charged with†—it means that the judge or jury can’t detect guilt.\n\n\n\n\n\n\nNoteDifferent evidentiary standards\n\n\n\nMany legal systems have different levels of evidentiary standards:\n\nStandards of proof in most common law systems (juries):  \n\nBalance of probabilities (civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nEvidentiary thresholds in the United States (juries): \n\nPreponderance of the evidence (civil cases)\nClear and convincing evidence (more important civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nStandards of proof in China (judges):   \n\n高度盖然性 [gāo dù gài rán xìng] / highly probable (civil cases)\n证据确实充分 [zhèng jù què shí chōng fēn] / facts being clear and evidence being sufficient | the evidence is definite and sufficient (criminal cases)\n\nLevels of doubt in Sharia systems (judges):     \n\nغلبة الظن [ghalabat al-zann] / preponderance of assumption (ta’zir cases and family matters)\nاليقين [yaqin] / certainty (hudud/qisas cases)\n\nStandard of proof in the International Criminal Court (judges): \n\nBeyond reasonable doubt (genocide, crimes against humanity, or war crimes)"
  },
  {
    "objectID": "diff-in-props.html#penguin-sex-ratios-across-species",
    "href": "diff-in-props.html#penguin-sex-ratios-across-species",
    "title": "Difference in proportions",
    "section": "Penguin sex ratios across species",
    "text": "Penguin sex ratios across species\nFor this example, we want to know if the proportion of female penguins is the same in Adelie and Gentoo species. Here’s what the sex breakdown looks like:\n\n\n\n\n\n\n\n\n\nWe can look at this more officially. First, we’ll load some packages:\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(parameters)\n\npenguins &lt;- penguins |&gt; drop_na(sex)\nThe proportions look pretty similar across the two species:\n\npenguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Gentoo\")) |&gt;\n  count(species, sex) |&gt;\n  group_by(species) |&gt;\n  mutate(proportion = n / sum(n)) |&gt;\n  filter(sex == \"female\")\n\n# A tibble: 2 × 4\n# Groups:   species [2]\n  species sex        n proportion\n  &lt;fct&gt;   &lt;fct&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie  female    73      0.5  \n2 Gentoo  female    58      0.487\n\n\nIs there actually a difference, or is it just noise? We need to do some hypothesis testing."
  },
  {
    "objectID": "diff-in-props.html#null-hypothesis-inference-with-infer",
    "href": "diff-in-props.html#null-hypothesis-inference-with-infer",
    "title": "Difference in proportions",
    "section": "Null hypothesis inference with {infer}",
    "text": "Null hypothesis inference with {infer}\n\nStep 1: Calculate \\(\\delta\\)Step 2: Simulate null worldStep 3: Put \\(\\delta\\) in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic we’re interested in is the difference in the proportion of female penguins between Adelie and Gentoo species.\n\ndelta &lt;- penguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Gentoo\")) |&gt;\n  specify(sex ~ species, success = \"female\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"Adelie\", \"Gentoo\"))\ndelta\n\nResponse: sex (factor)\nExplanatory: species (factor)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 0.0126\n\n\nThe difference in proportions is 0.013 (or 1.3 percentage points).\n\n\nWe create a null distribution by shuffling (or “permuting”) the species labels. This simulates a world where all the observed sexes are still the same, but where species assignment doesn’t matter. This eliminates all differences between the species.\n\nshuffled_data &lt;- penguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Gentoo\")) |&gt;\n  specify(sex ~ species, success = \"female\") |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 5000, type = \"permute\")\n\nNext we calculate the difference in proportions in each of these 5,000 shuffled worlds:\n\nnull_world &lt;- shuffled_data |&gt;\n  calculate(stat = \"diff in props\", order = c(\"Adelie\", \"Gentoo\"))\nnull_world\n\nResponse: sex (factor)\nExplanatory: species (factor)\nNull Hypothesis: independence\n# A tibble: 5,000 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1 -0.0942\n 2         2 -0.140 \n 3         3  0.0279\n 4         4 -0.0484\n 5         5 -0.0484\n 6         6 -0.0484\n 7         7 -0.125 \n 8         8  0.150 \n 9         9 -0.0637\n10        10 -0.0789\n# ℹ 4,990 more rows\n\n\nHere’s what this null world looks like:\n\nnull_world |&gt;\n  visualize()\n\n\n\n\n\n\n\n\nNotice that the differences are centered around 0, reflecting a world where species doesn’t affect the sex ratio.\n\n\nNext we put δ inside that null world to see how comfortably it fits there.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = NULL)\n\n\n\n\n\n\n\n\nThe observed difference of 0.013 sits right in the middle of the null world. It doesn’t look extreme at all.\n\n\nWe can quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a difference in proportions at least that extreme in a world where the proportions are the same.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")\n\n\n\n\n\n\n\n\n\np_value &lt;- null_world |&gt;\n  get_p_value(obs_stat = delta, direction = \"two-sided\")\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.928\n\n\nThe p-value is 0.928. This means that in a world where there is no difference in sex ratios between the species, there is a 92.84% chance of seeing a difference in proportions at least as extreme as 0.013.\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nUsing an α of 0.05, the p-value is 0.928, which is greater than 0.05. We don’t have enough evidence to say that there’s a difference in sex ratios between Adelie and Gentoo penguins. The difference is not statistically significant.\nThis does not mean the proportions are identical—we just can’t distinguish the difference from random noise with this sample.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")"
  },
  {
    "objectID": "diff-in-props.html#null-hypothesis-inference-with-prop.test",
    "href": "diff-in-props.html#null-hypothesis-inference-with-prop.test",
    "title": "Difference in proportions",
    "section": "Null hypothesis inference with prop.test()",
    "text": "Null hypothesis inference with prop.test()\nIn practice, most people do not simulate null worlds. Instead, they use a proportion test (prop.test()), which approximates the null world mathematically using a χ² distribution. The intuition is the same: a p-value is still the probability of seeing a difference at least that extreme in a world where the proportions are equal.\n\ntab &lt;- penguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Gentoo\")) |&gt;\n  mutate(species = fct_drop(species)) |&gt;\n  count(species, sex) |&gt;\n  pivot_wider(names_from = sex, values_from = n) |&gt;\n  column_to_rownames(\"species\") |&gt;\n  as.matrix()\n\nprop.test(tab)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  tab\nX-squared = 0.0065013, df = 1, p-value = 0.9357\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.1160296  0.1412397\nsample estimates:\n  prop 1   prop 2 \n0.500000 0.487395 \n\n\nBuried in that output is the p-value: 0.936. That’s huge. In a world where the two species have the same sex ratios, there’s a 93.57% probability of seeing a difference in proportions of 1.3 percentage points. We don’t have enough evidence to declare that there’s a difference between the two species. That doesn’t necessarily mean that there’s no difference. It means that if there really were a difference, we wouldn’t be able to detect it.\nIf you don’t like all that text output, you can feed the results of prop.test() to the model_parameters() function from the {parameters} package:\n\nprop.test(tab) |&gt;\n  model_parameters() |&gt;\n  display(caption = \"\")\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                Proportion\n                Difference\n                95% CI\n                Chi2(1)\n                p\n              \n        \n        \nAlternative hypothesis: two.sided\n\n        \n                \n                  50.00% / 48.74%\n                  1.26%\n                  (-0.12, 0.14)\n                  6.50e-03\n                  0.936"
  },
  {
    "objectID": "diff-in-props.html#footnotes",
    "href": "diff-in-props.html#footnotes",
    "title": "Difference in proportions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKind of—in common law systems, defendants are presumed innocent until\nproven guilty, so if there’s not enough evidence to prove guilt, they\nare innocent by definition. ↩︎"
  },
  {
    "objectID": "diff-in-means.html",
    "href": "diff-in-means.html",
    "title": "Difference in means",
    "section": "",
    "text": "◎◉○"
  },
  {
    "objectID": "diff-in-means.html#live-simulation",
    "href": "diff-in-means.html#live-simulation",
    "title": "Difference in means",
    "section": "Live simulation",
    "text": "Live simulation\n\n\njstat = require(\"jstat@1.9.6\")\n\nclrs = ({\n  gold: \"#f3d567\",\n  orange: \"#ee9b43\",\n  coral: \"#e74b47\",\n  crimson: \"#b80422\",\n  navy: \"#172767\",\n  teal: \"#19798b\"\n})\n\nfunction statLabel(value, textFn, dy, nullValues) {\n  const extent = d3.extent([...nullValues, value]);\n  const range = extent[1] - extent[0];\n  const pos = range === 0 ? 0.5 :\n    (value - extent[0]) / range;\n  const textAnchor = pos &gt; 0.82 ? \"end\" :\n    pos &lt; 0.18 ? \"start\" : \"middle\";\n  const dx = textAnchor === \"end\" ? -10 :\n    textAnchor === \"start\" ? 10 : 0;\n\n  const common = {\n    x: d =&gt; d, frameAnchor: \"top\", dy, dx,\n    text: textFn,\n    fontWeight: \"bold\", fontSize: 14,\n    textAnchor, paintOrder: \"stroke\"\n  };\n\n  return [\n    Plot.text([value], {\n      ...common,\n      stroke: \"white\", strokeWidth: 4, fill: \"black\"\n    })\n  ];\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof sample_size = Inputs.range([30, 2000], {\n  step: 10,\n  value: 100,\n  label: \"Sample size:\"\n})\n\nviewof effect_size = Inputs.range([-10, 10], {\n  step: 1,\n  value: 5,\n  label: \"Difference:\"\n})\n\nviewof spread = Inputs.range([1, 30], {\n  step: 1,\n  value: 10,\n  label: \"Spread (σ):\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_data = {\n  const data = [];\n  for (let i = 0; i &lt; sample_size; i++) {\n    const group = Math.random() &lt; 0.5 ? \"A\" : \"B\";\n    const mean = group === \"A\" ? 50 : 50 + effect_size;\n    data.push({\n      group,\n      outcome: jstat.normal.sample(mean, spread)\n    });\n  }\n  return data;\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  marginLeft: 50,\n  height: 280,\n  x: { label: \"Outcome\" },\n  y: { label: null },\n  fy: { label: null, domain: [\"A\", \"B\"] },\n  color: {\n    domain: [\"A\", \"B\"],\n    range: [clrs.navy, clrs.orange]\n  },\n  marks: [\n    Plot.rectY(\n      sample_data,\n      Plot.binX(\n        { y: \"count\" },\n        {\n          x: \"outcome\",\n          fill: \"group\",\n          fy: \"group\",\n          thresholds: 20\n        }\n      )\n    ),\n    Plot.ruleX(\n      [\n        { value: group_stats.meanA, group: \"A\" },\n        { value: group_stats.meanB, group: \"B\" }\n      ],\n      {\n        x: \"value\",\n        fy: \"group\",\n        stroke: \"black\",\n        strokeWidth: 2.5,\n        strokeDasharray: \"4,3\"\n      }\n    ),\n    Plot.text(\n      [\n        { value: group_stats.meanA, group: \"A\",\n          label: group_stats.meanA.toFixed(1) },\n        { value: group_stats.meanB, group: \"B\",\n          label: group_stats.meanB.toFixed(1) }\n      ],\n      {\n        x: \"value\",\n        fy: \"group\",\n        text: \"label\",\n        frameAnchor: \"top\",\n        dy: 12,\n        fontSize: 14,\n        fontWeight: \"bold\",\n        fill: \"black\",\n        stroke: \"white\",\n        strokeWidth: 4,\n        paintOrder: \"stroke\"\n      }\n    ),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Calculate δStep 2: Simulate null worldStep 3: Put δ in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic (δ) is the difference in means between the two groups.\n\ngroup_stats = {\n  const groups = { A: [], B: [] };\n  for (const d of sample_data) {\n    groups[d.group].push(d.outcome);\n  }\n\n  const meanA = d3.mean(groups.A);\n  const meanB = d3.mean(groups.B);\n  const nA = groups.A.length;\n  const nB = groups.B.length;\n\n  const seA = d3.deviation(groups.A) / Math.sqrt(nA);\n  const seB = d3.deviation(groups.B) / Math.sqrt(nB);\n  const ciA = [\n    meanA - 1.96 * seA,\n    meanA + 1.96 * seA\n  ];\n  const ciB = [\n    meanB - 1.96 * seB,\n    meanB + 1.96 * seB\n  ];\n\n  const diff = meanB - meanA;\n  const seDiff = Math.sqrt(\n    seA * seA + seB * seB\n  );\n  const ciDiff = [\n    diff - 1.96 * seDiff,\n    diff + 1.96 * seDiff\n  ];\n\n  return {\n    meanA, meanB, nA, nB, diff,\n    ciA, ciB, ciDiff\n  };\n}\n\nobs_stat = group_stats.diff\n\nobs_stat_abs = Math.abs(obs_stat).toFixed(2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference in means is .\n\nhtml`&lt;table class=\"table table-sm\" style=\"max-width: 500px;\"&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Group&lt;/th&gt;\n      &lt;th&gt;N&lt;/th&gt;\n      &lt;th&gt;Average&lt;/th&gt;\n      &lt;th&gt;95% CI&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr style=\"background-color: ${clrs.navy}11;\"&gt;\n      &lt;td&gt;A&lt;/td&gt;\n      &lt;td&gt;${group_stats.nA}&lt;/td&gt;\n      &lt;td&gt;${group_stats.meanA.toFixed(2)}&lt;/td&gt;\n      &lt;td&gt;[${group_stats.ciA[0].toFixed(2)}, ${group_stats.ciA[1].toFixed(2)}]&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=\"background-color: ${clrs.orange}22;\"&gt;\n      &lt;td&gt;B&lt;/td&gt;\n      &lt;td&gt;${group_stats.nB}&lt;/td&gt;\n      &lt;td&gt;${group_stats.meanB.toFixed(2)}&lt;/td&gt;\n      &lt;td&gt;[${group_stats.ciB[0].toFixed(2)}, ${group_stats.ciB[1].toFixed(2)}]&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=\"background-color: ${clrs.crimson}; color: white; font-weight: bold;\"&gt;\n      &lt;td&gt;Difference (B − A)&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n      &lt;td&gt;${group_stats.diff.toFixed(2)}&lt;/td&gt;\n      &lt;td&gt;[${group_stats.ciDiff[0].toFixed(2)}, ${group_stats.ciDiff[1].toFixed(2)}]&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;`\n\n\n\n\n\n\n\n\nWe create a null distribution by shuffling (or “permuting” to use the official stats term) the group labels. This simulates a world where all the real, measured values are still the same, but where group assignment doesn’t matter. This eliminates all differences between the groups.\nThink of this as being a world where there are no differences between the two groups. Importantly, this doesn’t mean that the measured difference between the groups is exactly 0. There is variation in the data, and that variation is reflected in the null world. What it means is that in the null world, the difference between the two groups is 0 ± some amount.\nHere’s what one shuffle looks like. Notice that the outcome values stay the same—only the group labels get reassigned:\n\nviewof reshuffle = Inputs.button(\"Reshuffle\")\n\n\n\n\n\n\n\nshuffle_preview = {\n  reshuffle;\n  const subset = sample_data.slice(0, 8);\n  const groups = subset.map(d =&gt; d.group);\n  const shuffled = groups.slice();\n  for (let i = shuffled.length - 1; i &gt; 0; i--) {\n    const j = Math.floor(Math.random() * (i + 1));\n    [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];\n  }\n  return {\n    original: subset.map((d, i) =&gt; ({\n      \" \": i + 1,\n      Group: d.group,\n      Outcome: +d.outcome.toFixed(1)\n    })),\n    shuffled: subset.map((d, i) =&gt; ({\n      \" \": i + 1,\n      Group: shuffled[i],\n      Outcome: +d.outcome.toFixed(1)\n    }))\n  };\n}\n\n\n\n\n\n\n\n\nOriginal data\n\nInputs.table(shuffle_preview.original, {\n  columns: [\" \", \"Group\", \"Outcome\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nShuffled data\n\nInputs.table(shuffle_preview.shuffled, {\n  columns: [\" \", \"Group\", \"Outcome\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nWhen we do this shuffle hundreds of times and compute the difference in means each time, we get a null distribution—a picture of what differences look like in a world where groups don’t matter.\nHere’s what this null world looks like:\n\nviewof n_reps = Inputs.range([100, 2000], {\n  step: 100,\n  value: 500,\n  label: \"Number of simulations:\"\n})\n\n\n\n\n\n\n\n\nnull_dist = {\n  const outcomes = sample_data.map(d =&gt; d.outcome);\n  const groups = sample_data.map(d =&gt; d.group);\n  const n = outcomes.length;\n  const results = [];\n\n  for (let r = 0; r &lt; n_reps; r++) {\n    // Fisher-Yates shuffle of group labels\n    const shuffled = groups.slice();\n    for (let i = n - 1; i &gt; 0; i--) {\n      const j = Math.floor(Math.random() * (i + 1));\n      [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];\n    }\n\n    let sumA = 0, nA = 0, sumB = 0, nB = 0;\n    for (let i = 0; i &lt; n; i++) {\n      if (shuffled[i] === \"B\") {\n        sumB += outcomes[i];\n        nB++;\n      } else {\n        sumA += outcomes[i];\n        nA++;\n      }\n    }\n    results.push({ stat: sumB / nB - sumA / nA });\n  }\n  return results;\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Difference (B − A)\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext we put δ inside that null world and see how comfortably it fits there.\nIs it surprising to see the red line in this null world? Is the line way out to one of the sides, or is it near the middle with the rest of the null world?\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Difference (B − A)\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    ),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${d.toFixed(2)}`, 20,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\nWe can actually quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a δ at least that extreme in a world where there’s no difference between the group averages.\n\n\np_value = {\n  const abs_obs = Math.abs(obs_stat);\n  const extreme = null_dist.filter(\n    d =&gt; Math.abs(d.stat) &gt;= abs_obs\n  ).length;\n  return extreme / null_dist.length;\n}\n\np_value_clean = p_value === 0\n  ? \"&lt; 0.001\"\n  : p_value.toFixed(3)\n\np_percent = p_value === 0\n  ? \"&lt; 0.1%\"\n  : (p_value * 100).toFixed(1) + \"%\"\n\n// Pre-bin the null distribution so entire bins get colored\nnull_bins = {\n  const values = null_dist.map(d =&gt; d.stat);\n  const bin = d3.bin().thresholds(30);\n  const bins = bin(values);\n  const abs_obs = Math.abs(obs_stat);\n  return bins.map(b =&gt; ({\n    x0: b.x0,\n    x1: b.x1,\n    count: b.length,\n    midpoint: (b.x0 + b.x1) / 2,\n    extreme: Math.abs((b.x0 + b.x1) / 2) &gt;= abs_obs\n  }));\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Difference (B − A)\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${d.toFixed(2)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe p-value is \n\nhtml`&lt;p&gt;This means that in a world where there is no difference between the groups, there is a &lt;strong&gt;${p_percent}&lt;/strong&gt; chance of seeing a difference of at least &lt;strong&gt;${obs_stat_abs}&lt;/strong&gt;&lt;/p&gt;`\n\n\n\n\n\n\n\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nThere are lots of possible thresholds. By convention, most people use a threshold (often shortened to \\(\\alpha\\)) of 0.05, or 5%. But that’s not required! You could have a lower standard with an \\(\\alpha\\) of 0.1 (10%), or a higher standard with an \\(\\alpha\\) of 0.01 (1%).\n\nviewof alpha = Inputs.select([0.10, 0.05, 0.01], {\n  label: \"Significance threshold (α):\",\n  value: 0.05\n})\n\n\n\n\n\n\n\n\n\n{\n  if (p_value &lt; alpha) {\n    return html`&lt;div class=\"alert alert-success\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where there is no difference between the groups, the probability of seeing a difference of at least &lt;strong&gt;${obs_stat_abs}&lt;/strong&gt; is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is less than ${alpha}, we have enough evidence to say that the difference is &lt;strong&gt;statistically significant.&lt;/strong&gt;&lt;/p&gt;\n    &lt;/div&gt;`;\n  } else {\n    return html`&lt;div class=\"alert alert-warning\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Not statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where there is no difference between the groups, the probability of seeing a difference of at least &lt;strong&gt;${obs_stat_abs}&lt;/strong&gt; is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is greater than ${alpha}, we don't have enough evidence to say that the difference doesn't come from the null world. The difference is thus &lt;strong&gt;not statistically significant.&lt;/strong&gt;&lt;/p&gt;\n      &lt;hr&gt;\n      &lt;p style=\"font-size: 0.9em;\"&gt;This does &lt;strong&gt;not&lt;/strong&gt; mean that there is no difference between the groups! We just don't have enough evidence to judge if there's a difference.&lt;/p&gt;\n    &lt;/div&gt;`;\n  }\n}\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  x: { label: \"Difference (B − A)\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `δ = ${d.toFixed(2)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\nEvidentiary standards\nWhen thinking about p-values and thresholds, I like to imagine myself as a judge or a member of a jury. Many legal systems around the world have formal evidentiary thresholds or standards of proof. If prosecutors provide evidence that meets a threshold (i.e. goes beyond a reasonable doubt, or shows evidence on a balance of probabilities), the judge or jury can rule guilty. If there’s not enough evidence to clear the standard or threshold, the judge or jury has to rule not guilty.\nWith p-values:\n\nIf the probability of seeing an effect or difference (or δ) in a null world is less than 5% (or whatever the threshold is), we rule it statistically significant and say that the difference does not fit in that world. We’re pretty confident that it’s not zero.\nIf the p-value is larger than the threshold, we do not have enough evidence to claim that δ doesn’t come from a world of where there’s no difference. We don’t know if it’s not zero.\n\nImportantly, if the difference is not significant, that does not mean that there is no difference. It just means that we can’t detect one if there is. If a prosecutor doesn’t provide sufficient evidence to clear a standard or threshold, it does not mean that the defendant didn’t do whatever they’re charged with†—it means that the judge or jury can’t detect guilt.\n\n\n\n\n\n\nNoteDifferent evidentiary standards\n\n\n\nMany legal systems have different levels of evidentiary standards:\n\nStandards of proof in most common law systems (juries):  \n\nBalance of probabilities (civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nEvidentiary thresholds in the United States (juries): \n\nPreponderance of the evidence (civil cases)\nClear and convincing evidence (more important civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nStandards of proof in China (judges):   \n\n高度盖然性 [gāo dù gài rán xìng] / highly probable (civil cases)\n证据确实充分 [zhèng jù què shí chōng fēn] / facts being clear and evidence being sufficient | the evidence is definite and sufficient (criminal cases)\n\nLevels of doubt in Sharia systems (judges):     \n\nغلبة الظن [ghalabat al-zann] / preponderance of assumption (ta’zir cases and family matters)\nاليقين [yaqin] / certainty (hudud/qisas cases)\n\nStandard of proof in the International Criminal Court (judges): \n\nBeyond reasonable doubt (genocide, crimes against humanity, or war crimes)"
  },
  {
    "objectID": "diff-in-means.html#differences-in-penguin-weights",
    "href": "diff-in-means.html#differences-in-penguin-weights",
    "title": "Difference in means",
    "section": "Differences in penguin weights",
    "text": "Differences in penguin weights\nFor this example, we’ll look at the body mass of penguins near Palmer Station, Antarctica. At first glance, it looks like there are some definite species-based differences in average penguin weight:\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                Species\n                Average weight (g)\n              \n        \n        \n        \n                \n                  Adelie\n                  3706\n                \n                \n                  Chinstrap\n                  3733\n                \n                \n                  Gentoo\n                  5092\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nLet’s look specifically at the difference in the average body mass for just Chinstrap and Gentoo penguins. First, we’ll load some packages:\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(parameters)\n\npenguins &lt;- penguins |&gt; drop_na(sex)\nWith some filtering, grouping, and summarizing, we can find the difference in means and see that Chinstraps seem to be a lot lighter than Gentoos, on average:\n\npenguins |&gt;\n  filter(species %in% c(\"Chinstrap\", \"Gentoo\")) |&gt;\n  group_by(species) |&gt;\n  summarize(avg_weight = mean(body_mass)) |&gt;\n  mutate(difference = c(NA, avg_weight[1] - avg_weight[2]))\n\n# A tibble: 2 × 3\n  species   avg_weight difference\n  &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Chinstrap      3733.        NA \n2 Gentoo         5092.     -1359.\n\n\nBut is that difference real? Could it potentially be zero? We need to do some hypothesis testing."
  },
  {
    "objectID": "diff-in-means.html#null-hypothesis-inference-with-infer",
    "href": "diff-in-means.html#null-hypothesis-inference-with-infer",
    "title": "Difference in means",
    "section": "Null hypothesis inference with {infer}",
    "text": "Null hypothesis inference with {infer}\n\nStep 1: Calculate \\(\\delta\\)Step 2: Simulate null worldStep 3: Put \\(\\delta\\) in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic we’re interested in is the difference in means between Chinstrap and Gentoo penguins.\nTo find this, we’ll filter the data to only keep those two species, use specify() to define the response and explanatory variables we’re using, and use calculate() to calculate the difference in means:\n\ndelta &lt;- penguins |&gt;\n  filter(species %in% c(\"Chinstrap\", \"Gentoo\")) |&gt;\n  specify(body_mass ~ species) |&gt;\n  calculate(stat = \"diff in means\", order = c(\"Chinstrap\", \"Gentoo\"))\ndelta\n\nResponse: body_mass (numeric)\nExplanatory: species (factor)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 -1359.\n\n\nThe difference in means between Chinstraps and Gentoos is −1,359 grams.\n\n\nWe create a null distribution by shuffling (or “permuting” to use the official stats term) the species label. This simulates a world where all the penguin weights are still the same, but where species assignment doesn’t matter. This eliminates all differences between the species.\n\nshuffled_data &lt;- penguins |&gt;\n  filter(species %in% c(\"Chinstrap\", \"Gentoo\")) |&gt;\n  specify(body_mass ~ species) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 5000, type = \"permute\")\nshuffled_data\n\nResponse: body_mass (numeric)\nExplanatory: species (factor)\nNull Hypothesis: independence\n# A tibble: 935,000 × 3\n# Groups:   replicate [5,000]\n   body_mass species replicate\n       &lt;dbl&gt; &lt;fct&gt;       &lt;int&gt;\n 1      4700 Gentoo          1\n 2      3300 Gentoo          1\n 3      5650 Gentoo          1\n 4      4875 Gentoo          1\n 5      4450 Gentoo          1\n 6      4375 Gentoo          1\n 7      4625 Gentoo          1\n 8      5050 Gentoo          1\n 9      5500 Gentoo          1\n10      4925 Gentoo          1\n# ℹ 934,990 more rows\n\n\nThe resulting data frame has 935,000 rows! That’s because we made 5,000 versions of the original data for 187 Chinstrap and Gentoo penguins.\nNext we need to calculate the \\(\\delta\\) in each of these 5,000 worlds:\n\nnull_world &lt;- shuffled_data |&gt;\n  calculate(stat = \"diff in means\", order = c(\"Chinstrap\", \"Gentoo\"))\nnull_world\n\nResponse: body_mass (numeric)\nExplanatory: species (factor)\nNull Hypothesis: independence\n# A tibble: 5,000 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1  -37.5 \n 2         2  152.  \n 3         3  -85.5 \n 4         4   -9.19\n 5         5  112.  \n 6         6 -127.  \n 7         7  136.  \n 8         8 -244.  \n 9         9 -225.  \n10        10 -185.  \n# ℹ 4,990 more rows\n\n\nThe distribution of all these differences in means across the different simulated datasets creates a null world, or null distribution. Here’s what this null world looks like:\n\nnull_world |&gt;\n  visualize()\n\n\n\n\n\n\n\n\nImportantly, notice that the difference between species is not exactly 0. There is variation in the actual data, and that variation is reflected in the null world. In a world where there is no difference between the species, the difference is 0 ± some amount. In this case, it’s 0 ± ≈250ish grams.\n\n\nNext we put \\(\\delta\\) inside that null world to see how comfortably it fits there.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = NULL)\n\n\n\n\n\n\n\n\nThat’s way far to the right and doesn’t look likely at all. A difference of −1,359 g is really unlikely in a world where there’s no difference between the species.\n\n\nWe can actually quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a \\(\\delta\\) at least that extreme in a world where there’s no difference between the species averages.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")\n\n\n\n\n\n\n\n\n\np_value &lt;- null_world |&gt;\n  get_p_value(obs_stat = delta, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nThe p-value is &lt; 0.001. (It’s so tiny that get_p_value() reports it as 0, along with an ominous warning. Technically it’s not zero—it’s just really really tiny.)\nThis means that in a world where there is no difference between the groups, there is a &lt; 0.1% chance of seeing a difference of at least −1,359.\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nThere are lots of possible thresholds. By convention, most people use a threshold (often shortened to \\(\\alpha\\)) of 0.05, or 5%. But that’s not required! You could have a lower standard with an \\(\\alpha\\) of 0.1 (10%), or a higher standard with an \\(\\alpha\\) of 0.01 (1%). In this case, we’ll assume an \\(\\alpha\\) of 0.05.\nIn this case, the p-value is &lt; 0.001 and our threshold for \\(\\alpha\\) is 0.05.\nIn a world where there is no difference between the two species, the probability of seeing a difference as extreme as −1,359 is &lt; 0.1%.\nSince &lt; 0.001 is less than 0.05, we have enough evidence to say that the difference is statistically significant.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")"
  },
  {
    "objectID": "diff-in-means.html#null-hypothesis-inference-with-t.test",
    "href": "diff-in-means.html#null-hypothesis-inference-with-t.test",
    "title": "Difference in means",
    "section": "Null hypothesis inference with t.test()",
    "text": "Null hypothesis inference with t.test()\nIn practice, most people do not simulate null worlds. Instead, they’ll use a built-in test that uses a known theoretical distribution of what the null world is assumed to look like (like the t, F, and \\(\\chi^2\\) distributions), and calculate a p-value based on that null distribution. This theoretical, mathematical p-value is what you see in regular statistical output.\nEven though these values are not based on simulations, the intuition is the same: a p-value is still the probability of seeing a \\(\\delta\\) at least that extreme in a world where there is no difference.\nTo find the difference in group means, we can conduct a t-test with t.test():\n\nt.test(\n  body_mass ~ species,\n  data = filter(penguins, species %in% c(\"Chinstrap\", \"Gentoo\"))\n)\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass by species\nt = -20.765, df = 169.62, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Chinstrap and group Gentoo is not equal to 0\n95 percent confidence interval:\n -1488.578 -1230.120\nsample estimates:\nmean in group Chinstrap    mean in group Gentoo \n               3733.088                5092.437 \n\n\nBuried in that giant wall of text is the p-value: p &lt; 2.2e-16, or p &lt; 2.2 × 10−16. That’s really tiny. In a world where Chinstrap and Gentoo penguins had the same average body mass, it would be virtually impossible to see a difference as extreme as −1,359. We have enough evidence to declare that difference is statistically significant.\nIf you don’t like all that text output, you can feed the results of t.test() to the model_parameters() function from the {parameters} package. This creates a nice little table with the different group means, the difference, the confidence interval, and the p-value. Feed that into display() and you’ll get a nicely rendered table.\n\nt.test(\n  body_mass ~ species,\n  data = filter(penguins, species %in% c(\"Chinstrap\", \"Gentoo\"))\n) |&gt;\n  model_parameters() |&gt;\n  display(caption = \"\")\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                Parameter\n                Group\n                species = Chinstrap\n                species = Gentoo\n                Difference\n                95% CI\n                t(169.62)\n                p\n              \n        \n        \nAlternative hypothesis: true difference in means between group Chinstrap and group Gentoo is not equal to 0\n\n        \n                \n                  body_mass\n                  species\n                  3733.09\n                  5092.44\n                  -1359.35\n                  (-1488.58, -1230.12)\n                  -20.76\n                  &lt; .001\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\nWarningSide caveat: Different “flavors” of tests\n\n\n\nSince we’re not simulating the null world, we’re making strong assumptions about what the null world looks like. In the case of this t-test, we’re assuming it follows a t-distribution with 169.6 degrees of freedom. That looks like this:\n\n\n\n\n\n\n\n\n\nWe can then put a standardized version of our observed \\(\\delta\\)—the t value of -20.8—in that mathematical null world and find the exact probability of seeing it there:\n\n\n\n\n\n\n\n\n\nWe can only assume that this precise t-distribution represents a null world under certain conditions. This is why there are so many statistical test flowcharts—you have to make sure you choose the test that matches the conditions and characteristics of your data.\nFor example, different versions of the t-test make different assumptions about whether the two groups have equal variances. By default, R uses Welch’s t-test, which is designed for group means with unequal variances, while the Student t-test assumes that group means have equal variances. Technically, in order to check which flavor of t-test we need to run, we’d need to test for equality of variances, which involves a separate statistical test with its own null hypothesis. If the variances are equal, we’d use t.test(..., var.equal = TRUE); if the variances are unequal, we’d use t.test(..., var.equal = FALSE). The only way to remember all these assumption checks and all the different versions of statistical tests is to consult a flowchart. It can be miserable.\nIf you simulate, you can skip all that. Your null world is based on the qualities of your observed data, not an idealized mathematical distribution."
  },
  {
    "objectID": "diff-in-means.html#footnotes",
    "href": "diff-in-means.html#footnotes",
    "title": "Difference in means",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKind of—in common law systems, defendants are presumed innocent until\nproven guilty, so if there’s not enough evidence to prove guilt, they\nare innocent by definition. ↩︎"
  },
  {
    "objectID": "evidentiary-standards.html",
    "href": "evidentiary-standards.html",
    "title": "Evidentiary standards",
    "section": "",
    "text": "◎◉○\nWhen thinking about p-values and thresholds, I like to imagine myself as a judge or a member of a jury. Many legal systems around the world have formal evidentiary thresholds or standards of proof. If prosecutors provide evidence that meets a threshold (i.e. goes beyond a reasonable doubt, or shows evidence on a balance of probabilities), the judge or jury can rule guilty. If there’s not enough evidence to clear the standard or threshold, the judge or jury has to rule not guilty.\nWith p-values:\nImportantly, if the difference is not significant, that does not mean that there is no difference. It just means that we can’t detect one if there is. If a prosecutor doesn’t provide sufficient evidence to clear a standard or threshold, it does not mean that the defendant didn’t do whatever they’re charged with†—it means that the judge or jury can’t detect guilt."
  },
  {
    "objectID": "evidentiary-standards.html#footnotes",
    "href": "evidentiary-standards.html#footnotes",
    "title": "Evidentiary standards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKind of—in common law systems, defendants are presumed innocent until\nproven guilty, so if there’s not enough evidence to prove guilt, they\nare innocent by definition. ↩︎"
  },
  {
    "objectID": "one-sample-mean.html",
    "href": "one-sample-mean.html",
    "title": "One-sample mean",
    "section": "",
    "text": "◎◉○"
  },
  {
    "objectID": "one-sample-mean.html#live-simulation",
    "href": "one-sample-mean.html#live-simulation",
    "title": "One-sample mean",
    "section": "Live simulation",
    "text": "Live simulation\n\n\njstat = require(\"jstat@1.9.6\")\n\nclrs = ({\n  gold: \"#f3d567\",\n  orange: \"#ee9b43\",\n  coral: \"#e74b47\",\n  crimson: \"#b80422\",\n  navy: \"#172767\",\n  teal: \"#19798b\"\n})\n\nfunction statLabel(value, textFn, dy, nullValues) {\n  const extent = d3.extent([...nullValues, value]);\n  const range = extent[1] - extent[0];\n  const pos = range === 0 ? 0.5 :\n    (value - extent[0]) / range;\n  const textAnchor = pos &gt; 0.82 ? \"end\" :\n    pos &lt; 0.18 ? \"start\" : \"middle\";\n  const dx = textAnchor === \"end\" ? -10 :\n    textAnchor === \"start\" ? 10 : 0;\n\n  const common = {\n    x: d =&gt; d, frameAnchor: \"top\", dy, dx,\n    text: textFn,\n    fontWeight: \"bold\", fontSize: 14,\n    textAnchor, paintOrder: \"stroke\"\n  };\n\n  return [\n    Plot.text([value], {\n      ...common,\n      stroke: \"white\", strokeWidth: 4, fill: \"black\"\n    })\n  ];\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof sample_size = Inputs.range([30, 2000], {\n  step: 10,\n  value: 100,\n  label: \"Sample size:\"\n})\n\nviewof comparison_value = Inputs.range([20, 80], {\n  step: 1,\n  value: 40,\n  label: \"Comparison value (μ₀):\"\n})\n\nviewof effect_size = Inputs.range([-10, 10], {\n  step: 1,\n  value: 5,\n  label: \"Difference from μ₀:\"\n})\n\nviewof spread = Inputs.range([1, 30], {\n  step: 1,\n  value: 10,\n  label: \"Spread (σ):\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_data = {\n  const data = [];\n  for (let i = 0; i &lt; sample_size; i++) {\n    data.push({\n      outcome: jstat.normal.sample(\n        comparison_value + effect_size, spread\n      )\n    });\n  }\n  return data;\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  marginLeft: 50,\n  height: 250,\n  x: { label: \"Outcome\" },\n  y: { label: null },\n  marks: [\n    Plot.rectY(\n      sample_data,\n      Plot.binX(\n        { y: \"count\" },\n        {\n          x: \"outcome\",\n          fill: clrs.navy,\n          fillOpacity: 0.7,\n          thresholds: 20\n        }\n      )\n    ),\n    Plot.ruleX([comparison_value], {\n      stroke: clrs.crimson,\n      strokeWidth: 2.5,\n      strokeDasharray: \"6,3\"\n    }),\n    Plot.text([comparison_value], {\n      x: d =&gt; d,\n      frameAnchor: \"top\",\n      dy: 12,\n      text: d =&gt; `μ₀ = ${d}`,\n      fill: clrs.crimson,\n      fontWeight: \"bold\",\n      fontSize: 14,\n      stroke: \"rgba(255,255,255,0.7)\",\n      strokeWidth: 5,\n      paintOrder: \"stroke\"\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Calculate δStep 2: Simulate null worldStep 3: Put δ in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic (δ) is the difference between the sample mean and the comparison value (μ₀).\n\nsample_stats = {\n  const values = sample_data.map(d =&gt; d.outcome);\n  const sample_mean = d3.mean(values);\n  const se = d3.deviation(values) / Math.sqrt(values.length);\n  const ci = [sample_mean - 1.96 * se, sample_mean + 1.96 * se];\n  const delta = sample_mean - comparison_value;\n\n  return { sample_mean, se, ci, delta, n: values.length };\n}\n\nobs_stat = sample_stats.sample_mean\ndelta = sample_stats.delta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;p&gt;The sample mean is &lt;strong&gt;${sample_stats.sample_mean.toFixed(2)}&lt;/strong&gt; and the comparison value is &lt;strong&gt;${comparison_value}&lt;/strong&gt;, so δ = &lt;strong&gt;${delta.toFixed(2)}&lt;/strong&gt;.&lt;/p&gt;`\n\n\n\n\n\n\n\nhtml`&lt;table class=\"table table-sm\" style=\"max-width: 500px;\"&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;Value&lt;/th&gt;\n      &lt;th&gt;95% CI&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr style=\"background-color: ${clrs.navy}11;\"&gt;\n      &lt;td&gt;Sample mean&lt;/td&gt;\n      &lt;td&gt;${sample_stats.sample_mean.toFixed(2)}&lt;/td&gt;\n      &lt;td&gt;[${sample_stats.ci[0].toFixed(2)}, ${sample_stats.ci[1].toFixed(2)}]&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=\"background-color: ${clrs.orange}22;\"&gt;\n      &lt;td&gt;Comparison value (μ₀)&lt;/td&gt;\n      &lt;td&gt;${comparison_value.toFixed(2)}&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=\"background-color: ${clrs.crimson}; color: white; font-weight: bold;\"&gt;\n      &lt;td&gt;Difference (δ)&lt;/td&gt;\n      &lt;td&gt;${delta.toFixed(2)}&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;`\n\n\n\n\n\n\n\n\nWe create a null distribution by recentering and bootstrapping. We shift our data so its average equals the comparison value (μ₀), then resample from that shifted data (with replacement) hundreds of times. This creates a world where the population mean is the comparison value.\nThink of this as being a world where the true mean is μ₀. Importantly, this doesn’t mean that every bootstrapped sample mean will be exactly μ₀. There is variation in the data, and that variation is reflected in the null world. What it means is that in the null world, the sample mean is μ₀ ± some amount.\nHere’s what the recentering and resampling looks like. The left table shows the original data shifted so its mean equals μ₀. The right table shows one bootstrap resample drawn from that shifted data (with replacement—notice some rows appear more than once):\n\nviewof resample = Inputs.button(\"Resample\")\n\n\n\n\n\n\n\nbootstrap_preview = {\n  resample;\n  const values = sample_data.map(d =&gt; d.outcome);\n  const sample_mean = d3.mean(values);\n  const shift = comparison_value - sample_mean;\n  const centered = values.map(v =&gt; v + shift);\n  const subset = centered.slice(0, 8);\n  const n = subset.length;\n\n  // Bootstrap resample from the subset\n  const resampled = [];\n  for (let i = 0; i &lt; n; i++) {\n    resampled.push(subset[Math.floor(Math.random() * n)]);\n  }\n\n  return {\n    centered: subset.map((v, i) =&gt; ({\n      \" \": i + 1,\n      Value: +v.toFixed(1)\n    })),\n    resampled: resampled.map((v, i) =&gt; ({\n      \" \": i + 1,\n      Value: +v.toFixed(1)\n    }))\n  };\n}\n\n\n\n\n\n\n\n\nRecentered data\n\nInputs.table(bootstrap_preview.centered, {\n  columns: [\" \", \"Value\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nBootstrap resample\n\nInputs.table(bootstrap_preview.resampled, {\n  columns: [\" \", \"Value\"],\n  rows: 8,\n  sort: false,\n  select: false\n})\n\n\n\n\n\n\n\n\nWhen we do this resampling hundreds of times and compute the mean each time, we get a null distribution—a picture of what sample means look like in a world where the true mean is μ₀.\nHere’s what this null world looks like:\n\nviewof n_reps = Inputs.range([100, 2000], {\n  step: 100,\n  value: 500,\n  label: \"Number of simulations:\"\n})\n\n\n\n\n\n\n\n\nnull_dist = {\n  // Recenter data around comparison value\n  const values = sample_data.map(d =&gt; d.outcome);\n  const sample_mean = d3.mean(values);\n  const shift = comparison_value - sample_mean;\n  const centered = values.map(v =&gt; v + shift);\n\n  const n = centered.length;\n  const results = [];\n\n  for (let r = 0; r &lt; n_reps; r++) {\n    // Bootstrap: sample with replacement\n    let sum = 0;\n    for (let i = 0; i &lt; n; i++) {\n      sum += centered[Math.floor(Math.random() * n)];\n    }\n    results.push({ stat: sum / n });\n  }\n  return results;\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Bootstrapped mean\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext we put our observed sample mean inside that null world and see how comfortably it fits there.\nIs it surprising to see the red line in this null world? Is the line way out to one of the sides, or is it near the middle with the rest of the null world?\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Sample mean\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(\n      null_dist,\n      Plot.binX(\n        { y: \"count\" },\n        { x: \"stat\", fill: clrs.navy, fillOpacity: 0.7 }\n      )\n    ),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `x̄ = ${d.toFixed(2)}`, 20,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\nWe can actually quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a sample mean at least that far from μ₀ in a world where the true mean is μ₀.\n\n\np_value = {\n  const abs_dist = Math.abs(obs_stat - comparison_value);\n  const extreme = null_dist.filter(\n    d =&gt; Math.abs(d.stat - comparison_value) &gt;= abs_dist\n  ).length;\n  return extreme / null_dist.length;\n}\n\np_value_clean = p_value === 0\n  ? \"&lt; 0.001\"\n  : p_value.toFixed(3)\n\np_percent = p_value === 0\n  ? \"&lt; 0.1%\"\n  : (p_value * 100).toFixed(1) + \"%\"\n\nnull_bins = {\n  const values = null_dist.map(d =&gt; d.stat);\n  const bin = d3.bin().thresholds(30);\n  const bins = bin(values);\n  const abs_dist = Math.abs(obs_stat - comparison_value);\n  return bins.map(b =&gt; ({\n    x0: b.x0,\n    x1: b.x1,\n    count: b.length,\n    extreme: Math.abs(((b.x0 + b.x1) / 2) - comparison_value)\n      &gt;= abs_dist\n  }));\n}\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  width: 500,\n  x: { label: \"Sample mean\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `x̄ = ${d.toFixed(2)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe p-value is \n\nhtml`&lt;p&gt;This means that in a world where the true mean is &lt;strong&gt;${comparison_value}&lt;/strong&gt;, there is a &lt;strong&gt;${p_percent}&lt;/strong&gt; chance of seeing a sample mean at least as far as &lt;strong&gt;${obs_stat.toFixed(2)}&lt;/strong&gt; from μ₀&lt;/p&gt;`\n\n\n\n\n\n\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nThere are lots of possible thresholds. By convention, most people use a threshold (often shortened to α) of 0.05, or 5%. But that’s not required! You could have a lower standard with an α of 0.1 (10%), or a higher standard with an α of 0.01 (1%).\n\nviewof alpha = Inputs.select([0.10, 0.05, 0.01], {\n  label: \"Significance threshold (α):\",\n  value: 0.05\n})\n\n\n\n\n\n\n\n\n\n{\n  if (p_value &lt; alpha) {\n    return html`&lt;div class=\"alert alert-success\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where the true mean is &lt;strong&gt;${comparison_value}&lt;/strong&gt;, the probability of seeing a sample mean at least as far as &lt;strong&gt;${obs_stat.toFixed(2)}&lt;/strong&gt; from μ₀ is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is less than ${alpha}, we have enough evidence to say that the difference is &lt;strong&gt;statistically significant.&lt;/strong&gt;&lt;/p&gt;\n    &lt;/div&gt;`;\n  } else {\n    return html`&lt;div class=\"alert alert-warning\" role=\"alert\"&gt;\n      &lt;h5 class=\"alert-heading\"&gt;Not statistically significant&lt;/h5&gt;\n      &lt;p&gt;The p-value is &lt;strong&gt;${p_value_clean}&lt;/strong&gt; and our threshold for α is &lt;strong&gt;${alpha}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;In a world where the true mean is &lt;strong&gt;${comparison_value}&lt;/strong&gt;, the probability of seeing a sample mean at least as far as &lt;strong&gt;${obs_stat.toFixed(2)}&lt;/strong&gt; from μ₀ is &lt;strong&gt;${p_percent}&lt;/strong&gt;&lt;/p&gt;\n      &lt;p&gt;Since ${p_value_clean} is greater than ${alpha}, we don't have enough evidence to say that the mean differs from μ₀. The difference is thus &lt;strong&gt;not statistically significant.&lt;/strong&gt;&lt;/p&gt;\n      &lt;hr&gt;\n      &lt;p style=\"font-size: 0.9em;\"&gt;This does &lt;strong&gt;not&lt;/strong&gt; mean that the true mean equals μ₀! We just don't have enough evidence to judge if it differs.&lt;/p&gt;\n    &lt;/div&gt;`;\n  }\n}\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  style: { fontSize: \"13px\" },\n  height: 300,\n  x: { label: \"Sample mean\" },\n  y: { label: \"Count\" },\n  marks: [\n    Plot.rectY(null_bins, {\n      x1: \"x0\",\n      x2: \"x1\",\n      y: \"count\",\n      fill: d =&gt; d.extreme ? clrs.coral + \"aa\" : clrs.navy + \"b3\",\n      stroke: \"white\",\n      strokeWidth: 0.5\n    }),\n    Plot.ruleX([obs_stat], { stroke: \"red\", strokeWidth: 3 }),\n    ...statLabel(obs_stat, d =&gt; `x̄ = ${d.toFixed(2)}`, 20,\n      null_dist.map(d =&gt; d.stat)),\n    ...statLabel(obs_stat, () =&gt; `p = ${p_value_clean}`, 45,\n      null_dist.map(d =&gt; d.stat))\n  ]\n})\n\n\n\n\n\n\n\n\n\nEvidentiary standards\nWhen thinking about p-values and thresholds, I like to imagine myself as a judge or a member of a jury. Many legal systems around the world have formal evidentiary thresholds or standards of proof. If prosecutors provide evidence that meets a threshold (i.e. goes beyond a reasonable doubt, or shows evidence on a balance of probabilities), the judge or jury can rule guilty. If there’s not enough evidence to clear the standard or threshold, the judge or jury has to rule not guilty.\nWith p-values:\n\nIf the probability of seeing an effect or difference (or δ) in a null world is less than 5% (or whatever the threshold is), we rule it statistically significant and say that the difference does not fit in that world. We’re pretty confident that it’s not zero.\nIf the p-value is larger than the threshold, we do not have enough evidence to claim that δ doesn’t come from a world of where there’s no difference. We don’t know if it’s not zero.\n\nImportantly, if the difference is not significant, that does not mean that there is no difference. It just means that we can’t detect one if there is. If a prosecutor doesn’t provide sufficient evidence to clear a standard or threshold, it does not mean that the defendant didn’t do whatever they’re charged with†—it means that the judge or jury can’t detect guilt.\n\n\n\n\n\n\nNoteDifferent evidentiary standards\n\n\n\nMany legal systems have different levels of evidentiary standards:\n\nStandards of proof in most common law systems (juries):  \n\nBalance of probabilities (civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nEvidentiary thresholds in the United States (juries): \n\nPreponderance of the evidence (civil cases)\nClear and convincing evidence (more important civil cases)\nBeyond a reasonable doubt (criminal cases)\n\nStandards of proof in China (judges):   \n\n高度盖然性 [gāo dù gài rán xìng] / highly probable (civil cases)\n证据确实充分 [zhèng jù què shí chōng fēn] / facts being clear and evidence being sufficient | the evidence is definite and sufficient (criminal cases)\n\nLevels of doubt in Sharia systems (judges):     \n\nغلبة الظن [ghalabat al-zann] / preponderance of assumption (ta’zir cases and family matters)\nاليقين [yaqin] / certainty (hudud/qisas cases)\n\nStandard of proof in the International Criminal Court (judges): \n\nBeyond reasonable doubt (genocide, crimes against humanity, or war crimes)"
  },
  {
    "objectID": "one-sample-mean.html#average-penguin-body-mass",
    "href": "one-sample-mean.html#average-penguin-body-mass",
    "title": "One-sample mean",
    "section": "Average penguin body mass",
    "text": "Average penguin body mass\nFor this example, we’ll use the body mass of all penguins near Palmer Station, Antarctica. We want to know if the average body mass is different from 4,000 grams.\nAt first glance, it looks like most penguins are already roughly around 4,000 grams. Some are lighter, a lot are heavier, but 4,000 seems fairly reasonable.\n\n\n\n\n\n\n\n\n\nWe can look at this more officially. First, we’ll load some packages and check the official average weight:\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(parameters)\n\npenguins &lt;- penguins |&gt; drop_na(sex)\n\npenguins |&gt;\n  summarize(avg_weight = mean(body_mass))\n\n  avg_weight\n1   4207.057\n\n\nThe sample mean is 4,207 g, which is a bit higher than our comparison value of 4,000 g. But is that difference real, or could it just be due to random chance?"
  },
  {
    "objectID": "one-sample-mean.html#null-hypothesis-inference-with-infer",
    "href": "one-sample-mean.html#null-hypothesis-inference-with-infer",
    "title": "One-sample mean",
    "section": "Null hypothesis inference with {infer}",
    "text": "Null hypothesis inference with {infer}\n\nStep 1: Calculate \\(\\delta\\)Step 2: Simulate null worldStep 3: Put \\(\\delta\\) in the null worldStep 4: p-valueStep 5: Decision\n\n\nThe sample statistic we’re interested in is the mean body mass.\n\ndelta &lt;- penguins |&gt;\n  specify(response = body_mass) |&gt;\n  calculate(stat = \"mean\")\ndelta\n\nResponse: body_mass (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 4207.\n\n\nThe sample mean is 4,207 grams. We’re comparing this to μ₀ = 4,000 g, so the difference (δ) is 207 grams.\n\n\nWe create a null distribution by recentering our data around μ₀ = 4,000 g and then bootstrapping (resampling with replacement) from that shifted data. This creates a world where the true population mean is 4,000 g.\n\nshuffled_data &lt;- penguins |&gt;\n  specify(response = body_mass) |&gt;\n  hypothesize(null = \"point\", mu = 4000) |&gt;\n  generate(reps = 5000, type = \"bootstrap\")\n\nNext we need to calculate the mean in each of these 5,000 resampled worlds:\n\nnull_world &lt;- shuffled_data |&gt;\n  calculate(stat = \"mean\")\nnull_world\n\nResponse: body_mass (numeric)\nNull Hypothesis: point\n# A tibble: 5,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 4010.\n 2         2 4017.\n 3         3 3939.\n 4         4 3991.\n 5         5 3946.\n 6         6 3967.\n 7         7 3965.\n 8         8 4004.\n 9         9 3986.\n10        10 3930.\n# ℹ 4,990 more rows\n\n\nHere’s what this null world looks like:\n\nnull_world |&gt;\n  visualize()\n\n\n\n\n\n\n\n\nNotice that the null world is centered around 4,000 g—our comparison value—with variation reflecting what sample means would look like if the true mean really were 4,000 g.\n\n\nNext we put our observed sample mean inside that null world to see how comfortably it fits there.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = NULL)\n\n\n\n\n\n\n\n\nThat’s way out in the right tail. A sample mean of 4,207 g looks really unlikely in a world where the true mean is 4,000 g.\n\n\nWe can quantify the probability of seeing that red line in a null world. This is a p-value—the probability of seeing a sample mean at least that far from μ₀ in a world where the true mean is 4,000 g.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")\n\n\n\n\n\n\n\n\n\np_value &lt;- null_world |&gt;\n  get_p_value(obs_stat = delta, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nThe p-value is &lt; 0.001. This means that in a world where the true mean body mass is 4,000 g, there is a &lt; 0.1% chance of seeing a sample mean at least as far from 4,000 as 4,207.\n\n\nFinally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we aren’t in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).\nUsing an α of 0.05, the p-value is &lt; 0.001, which is less than 0.05. We have enough evidence to say that the average penguin body mass is statistically significantly different from 4,000 grams.\n\nnull_world |&gt;\n  visualize() +\n  shade_p_value(obs_stat = delta, direction = \"two-sided\")"
  },
  {
    "objectID": "one-sample-mean.html#null-hypothesis-inference-with-t.test",
    "href": "one-sample-mean.html#null-hypothesis-inference-with-t.test",
    "title": "One-sample mean",
    "section": "Null hypothesis inference with t.test()",
    "text": "Null hypothesis inference with t.test()\nIn practice, most people do not simulate null worlds. Instead, they use a one-sample t-test, which approximates the null world mathematically using a t-distribution. The intuition is the same: a p-value is still the probability of seeing a sample mean at least that extreme in a world where the true mean is μ₀.\nTo test whether the average body mass differs from 4,000 g, we can use t.test() with mu = 4000:\n\nt.test(penguins$body_mass, mu = 4000)\n\n\n    One Sample t-test\n\ndata:  penguins$body_mass\nt = 4.6925, df = 332, p-value = 3.952e-06\nalternative hypothesis: true mean is not equal to 4000\n95 percent confidence interval:\n 4120.256 4293.858\nsample estimates:\nmean of x \n 4207.057 \n\n\nThe p-value is in that wall of text—it’s 3.952e-06, or 3.952 × 10−6, or 0.000003952. That’s really tiny. In a world where the average weight for all penguins is around 4,000 grams, it would be virtually impossible to see a difference as extreme as 4,207. We have enough evidence to declare that difference is statistically significant.\nIf you don’t like all that text output, you can feed the results of t.test() to the model_parameters() function from the {parameters} package:\n\nt.test(penguins$body_mass, mu = 4000) |&gt;\n  model_parameters() |&gt;\n  display(caption = \"\")\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                Parameter\n                Mean\n                mu\n                Difference\n                95% CI\n                t(332)\n                p\n              \n        \n        \nAlternative hypothesis: true mean is not equal to 4000\n\n        \n                \n                  body_mass\n                  4207.06\n                  4000\n                  207.06\n                  (4120.26, 4293.86)\n                  4.69\n                  &lt; .001"
  },
  {
    "objectID": "one-sample-mean.html#footnotes",
    "href": "one-sample-mean.html#footnotes",
    "title": "One-sample mean",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKind of—in common law systems, defendants are presumed innocent until\nproven guilty, so if there’s not enough evidence to prove guilt, they\nare innocent by definition. ↩︎"
  }
]