---
title: "Regression slope"
---

::: {.column-screen .hero-banner-mini}

# Regression slope

◎◉○

:::

## Live simulation

::::::::: {.column-screen-inset}

```{ojs}
//| echo: false

jstat = require("jstat@1.9.6")

clrs = ({
  gold: "#f3d567",
  orange: "#ee9b43",
  coral: "#e74b47",
  crimson: "#b80422",
  navy: "#172767",
  teal: "#19798b",
  gray: "#4d4d4d"
})

// Simple OLS helper: returns { intercept, slope }
function ols(xvals, yvals) {
  const n = xvals.length;
  const mx = d3.mean(xvals);
  const my = d3.mean(yvals);
  let ssxy = 0, ssxx = 0;
  for (let i = 0; i < n; i++) {
    ssxy += (xvals[i] - mx) * (yvals[i] - my);
    ssxx += (xvals[i] - mx) * (xvals[i] - mx);
  }
  const slope = ssxy / ssxx;
  const intercept = my - slope * mx;
  return { intercept, slope };
}

function statLabel(value, textFn, dy, nullValues) {
  const extent = d3.extent([...nullValues, value]);
  const range = extent[1] - extent[0];
  const pos = range === 0 ? 0.5 :
    (value - extent[0]) / range;
  const textAnchor = pos > 0.82 ? "end" :
    pos < 0.18 ? "start" : "middle";
  const dx = textAnchor === "end" ? -10 :
    textAnchor === "start" ? 10 : 0;

  const common = {
    x: d => d, frameAnchor: "top", dy, dx,
    text: textFn,
    fontWeight: "bold", fontSize: 14,
    textAnchor, paintOrder: "stroke"
  };

  return [
    Plot.text([value], {
      ...common,
      stroke: "white", strokeWidth: 4, fill: "black"
    })
  ];
}
```

:::::::: {.grid}

::::::: {.g-col-12 .g-col-md-4 .sticky}

```{ojs}
//| echo: false

viewof sample_size = Inputs.range([30, 2000], {
  step: 10,
  value: 100,
  label: "Sample size:"
})

viewof slope_size = Inputs.range([-5, 5], {
  step: 0.1,
  value: 0.8,
  label: "Slope:"
})

viewof spread = Inputs.range([1, 15], {
  step: 1,
  value: 5,
  label: "Noise (σ):"
})
```

```{ojs}
//| echo: false

sample_data = {
  const data = [];
  for (let i = 0; i < sample_size; i++) {
    const x = Math.random() * 10;
    const y = 20 + slope_size * x +
      jstat.normal.sample(0, spread);
    data.push({ x, y });
  }
  return data;
}

obs_fit = ols(
  sample_data.map(d => d.x),
  sample_data.map(d => d.y)
)

Plot.plot({
  style: { fontSize: "13px"},
  marginLeft: 50,
  marginBottom: 45,
  width: 500,
  height: 350,
  x: { label: "Explanatory variable (x)", labelOffset: 35 },
  y: { label: "Outcome variable (y)" },
  marks: [
    Plot.dot(sample_data, {
      x: "x",
      y: "y",
      fill: clrs.navy,
      r: 3
    }),
    Plot.line(
      [
        { x: 0, y: obs_fit.intercept },
        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }
      ],
      {
        x: "x",
        y: "y",
        stroke: clrs.crimson,
        strokeWidth: 2.5
      }
    )
  ]
})
```

:::::::

::::::: {.g-col-12 .g-col-md-8}

:::::: {.panel-tabset .nav-pills .simulation}

## Step 1: Calculate δ

The sample statistic (δ) is the **regression slope**—the estimated change in y for a one-unit increase in x.

```{ojs}
//| echo: false

model_stats = {
  const xvals = sample_data.map(d => d.x);
  const yvals = sample_data.map(d => d.y);
  const fit = ols(xvals, yvals);
  const n = xvals.length;

  // Residual standard error
  const mx = d3.mean(xvals);
  let ssxx = 0, sse = 0;
  for (let i = 0; i < n; i++) {
    const pred = fit.intercept + fit.slope * xvals[i];
    sse += (yvals[i] - pred) ** 2;
    ssxx += (xvals[i] - mx) ** 2;
  }
  const rse = Math.sqrt(sse / (n - 2));
  const se_slope = rse / Math.sqrt(ssxx);
  const se_intercept = rse *
    Math.sqrt(1 / n + mx * mx / ssxx);

  return {
    intercept: fit.intercept,
    slope: fit.slope,
    se_slope,
    se_intercept,
    ci_slope: [
      fit.slope - 1.96 * se_slope,
      fit.slope + 1.96 * se_slope
    ],
    ci_intercept: [
      fit.intercept - 1.96 * se_intercept,
      fit.intercept + 1.96 * se_intercept
    ]
  };
}

obs_stat = model_stats.slope

obs_stat_abs = Math.abs(obs_stat).toFixed(3)
```

The regression slope is **`{ojs} obs_stat.toFixed(3)`**.

```{ojs}
//| echo: false

html`<table class="table table-sm" style="max-width: 500px;">
  <thead>
    <tr>
      <th>Term</th>
      <th>Estimate</th>
      <th>95% CI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>(Intercept)</td>
      <td>${model_stats.intercept.toFixed(3)}</td>
      <td>[${model_stats.ci_intercept[0].toFixed(3)}, ${model_stats.ci_intercept[1].toFixed(3)}]</td>
    </tr>
    <tr style="background-color: ${clrs.crimson}; color: white; font-weight: bold;">
      <td>x</td>
      <td>${model_stats.slope.toFixed(3)}</td>
      <td>[${model_stats.ci_slope[0].toFixed(3)}, ${model_stats.ci_slope[1].toFixed(3)}]</td>
    </tr>
  </tbody>
</table>`
```

## Step 2: Simulate null world

We create a null distribution by shuffling (or "permuting" to use the official stats term) the values of x. This simulates a world where all the real, measured values of both x and y are still the same, but where the relationship between x and y doesn't matter. **This eliminates any association between x and y.**

Think of this as being a world where there is no relationship between x and y. Importantly, this *doesn't* mean that the slope is *exactly* 0. There is variation in the data, and that variation is reflected in the null world. What it means is that in the null world, the slope is 0 ± some amount.

Here's what one shuffle looks like. Notice that the y values stay the same—only the x values get reassigned:

```{ojs}
//| echo: false

viewof reshuffle = Inputs.button("Reshuffle")
```

```{ojs}
//| echo: false

shuffle_preview = {
  reshuffle;
  const subset = sample_data.slice(0, 8);
  const xvals = subset.map(d => d.x);
  const shuffled_x = xvals.slice();
  for (let i = shuffled_x.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [shuffled_x[i], shuffled_x[j]] =
      [shuffled_x[j], shuffled_x[i]];
  }
  return {
    original: subset.map((d, i) => ({
      " ": i + 1,
      x: +d.x.toFixed(1),
      y: +d.y.toFixed(1)
    })),
    shuffled: subset.map((d, i) => ({
      " ": i + 1,
      x: +shuffled_x[i].toFixed(1),
      y: +d.y.toFixed(1)
    }))
  };
}
```

::::: {.grid .shuffled}

:::: {.g-col-12 .g-col-sm-6 .g-col-lg-5 .g-col-xl-4}

**Original data**

```{ojs}
//| echo: false

Inputs.table(shuffle_preview.original, {
  columns: [" ", "x", "y"],
  rows: 12,
  sort: false,
  select: false
})
```

::::

:::: {.g-col-12 .g-col-sm-6 .g-col-lg-5 .g-col-xl-4}

**Shuffled data**

```{ojs}
//| echo: false

Inputs.table(shuffle_preview.shuffled, {
  columns: [" ", "x", "y"],
  rows: 12,
  sort: false,
  select: false
})
```

::::

:::::

When we do this shuffle hundreds of times and fit a regression each time, we get a **null distribution**—a picture of what slopes look like in a world where x and y are unrelated.

Here's what this null world looks like:

```{ojs}
//| echo: false

viewof n_reps = Inputs.range([100, 2000], {
  step: 100,
  value: 500,
  label: "Number of simulations:"
})
```

::: {style="width: 75%;"}

```{ojs}
//| echo: false

null_results = {
  const xvals = sample_data.map(d => d.x);
  const yvals = sample_data.map(d => d.y);
  const n = xvals.length;
  const stats = [];
  const lines = [];

  for (let r = 0; r < n_reps; r++) {
    // Fisher-Yates shuffle of x values
    const shuffled_x = xvals.slice();
    for (let i = n - 1; i > 0; i--) {
      const j = Math.floor(Math.random() * (i + 1));
      [shuffled_x[i], shuffled_x[j]] =
        [shuffled_x[j], shuffled_x[i]];
    }

    const fit = ols(shuffled_x, yvals);
    stats.push({ stat: fit.slope });
    lines.push({
      intercept: fit.intercept,
      slope: fit.slope
    });
  }
  return { stats, lines };
}

null_dist = null_results.stats

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: { label: "Slope" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(
      null_dist,
      Plot.binX(
        { y: "count" },
        { x: "stat", fill: clrs.gray }
      )
    )
  ]
})
```

:::

Here's another way to see the null world slopes. Each thin line is a regression line for one of the simulated worlds where x and y are unrelated.

::: {style="width: 75%;"}

```{ojs}
//| echo: false

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: {
    label: "Explanatory variable (x)",
    domain: [0, 10]
  },
  y: { label: "Outcome variable (y)" },
  marks: [
    // Null world regression lines
    ...null_results.lines.map(l =>
      Plot.line(
        [
          { x: 0, y: l.intercept },
          { x: 10, y: l.intercept + l.slope * 10 }
        ],
        {
          x: "x",
          y: "y",
          stroke: "black",
          strokeWidth: 0.1,
          strokeOpacity: 0.3
        }
      )
    ),
    Plot.dot(sample_data, {
      x: "x",
      y: "y",
      fill: clrs.navy,
      fillOpacity: 0.7,
      r: 3
    })
  ]
})
```

:::

## Step 3: Put δ in the null world

Next we put δ inside that null world and see how comfortably it fits there.

Is it surprising to see the red line in this null world? Is the line way out to one of the sides, or is it near the middle with the rest of the null world?

::: {style="width: 75%;"}

```{ojs}
//| echo: false

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: { label: "Slope" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(
      null_dist,
      Plot.binX(
        { y: "count" },
        { x: "stat", fill: clrs.gray }
      )
    ),
    Plot.ruleX([obs_stat], { stroke: "red", strokeWidth: 3 }),
    ...statLabel(obs_stat, d => `δ = ${d.toFixed(3)}`, 20,
      null_dist.map(d => d.stat))
  ]
})
```

:::

Or alternatively, we can put the observed regression line from the actual data into the scatterplot with the null world regression lines. Is it surprising to see the red line in this null world?

::: {style="width: 75%;"}

```{ojs}
//| echo: false

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: {
    label: "Explanatory variable (x)",
    domain: [0, 10]
  },
  y: { label: "Outcome variable (y)" },
  marks: [
    ...null_results.lines.map(l =>
      Plot.line(
        [
          { x: 0, y: l.intercept },
          { x: 10, y: l.intercept + l.slope * 10 }
        ],
        {
          x: "x",
          y: "y",
          stroke: "black",
          strokeWidth: 0.1,
          strokeOpacity: 0.3
        }
      )
    ),
    Plot.dot(sample_data, {
      x: "x",
      y: "y",
      fill: clrs.navy,
      fillOpacity: 0.7,
      r: 3
    }),
    Plot.line(
      [
        { x: 0, y: obs_fit.intercept },
        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }
      ],
      {
        x: "x",
        y: "y",
        stroke: "red",
        strokeWidth: 3
      }
    )
  ]
})
```

:::

## Step 4: p-value

We can actually quantify the probability of seeing that red line in a null world. This is a **p-value**—the probability of seeing a δ at least that extreme in a world where there's no relationship between x and y.

::: {style="width: 75%;"}

```{ojs}
//| echo: false

p_value = {
  const abs_obs = Math.abs(obs_stat);
  const extreme = null_dist.filter(
    d => Math.abs(d.stat) >= abs_obs
  ).length;
  return extreme / null_dist.length;
}

p_value_clean = p_value === 0
  ? "< 0.001"
  : p_value.toFixed(3)

p_percent = p_value === 0
  ? "< 0.1%"
  : (p_value * 100).toFixed(1) + "%"

null_bins = {
  const values = null_dist.map(d => d.stat);
  const bin = d3.bin().thresholds(30);
  const bins = bin(values);
  const abs_obs = Math.abs(obs_stat);
  return bins.map(b => ({
    x0: b.x0,
    x1: b.x1,
    count: b.length,
    extreme: Math.abs((b.x0 + b.x1) / 2) >= abs_obs
  }));
}

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: { label: "Slope" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(null_bins, {
      x1: "x0",
      x2: "x1",
      y: "count",
      fill: d => d.extreme ? clrs.coral + "aa" : clrs.gray,
      stroke: "white",
      strokeWidth: 0.5
    }),
    Plot.ruleX([obs_stat], { stroke: "red", strokeWidth: 3 }),
    ...statLabel(obs_stat, d => `δ = ${d.toFixed(3)}`, 20,
      null_dist.map(d => d.stat)),
    ...statLabel(obs_stat, () => `p = ${p_value_clean}`, 45,
      null_dist.map(d => d.stat))
  ]
})
```

```{ojs}
//| echo: false

// Also show the slope plot with observed line
Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: {
    label: "Explanatory variable (x)",
    domain: [0, 10]
  },
  y: { label: "Outcome variable (y)" },
  marks: [
    ...null_results.lines.map(l =>
      Plot.line(
        [
          { x: 0, y: l.intercept },
          { x: 10, y: l.intercept + l.slope * 10 }
        ],
        {
          x: "x",
          y: "y",
          stroke: "black",
          strokeWidth: 0.1,
          strokeOpacity: 0.3
        }
      )
    ),
    Plot.dot(sample_data, {
      x: "x",
      y: "y",
      fill: clrs.navy,
      fillOpacity: 0.7,
      r: 3
    }),
    Plot.line(
      [
        { x: 0, y: obs_fit.intercept },
        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }
      ],
      {
        x: "x",
        y: "y",
        stroke: "red",
        strokeWidth: 3
      }
    )
  ]
})
```

:::

The p-value is **`{ojs} p_value_clean`**

```{ojs}
//| echo: false

html`<p>This means that in a world where there is no relationship between x and y, there is a <strong>${p_percent}</strong> chance of seeing a slope of at least <strong>${obs_stat_abs}</strong></p>`
```

## Step 5: Decision

Finally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we *aren't* in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).

There are lots of possible thresholds. By convention, most people use a threshold (often shortened to α) of 0.05, or 5%. But that's not required! You could have a lower standard with an α of 0.1 (10%), or a higher standard with an α of 0.01 (1%).

```{ojs}
//| echo: false

viewof alpha = Inputs.select([0.10, 0.05, 0.01], {
  label: "Significance threshold (α):",
  value: 0.05
})
```

::::: {.grid}

:::: {.g-col-12 .g-col-md-6}

```{ojs}
//| echo: false

{
  if (p_value < alpha) {
    return html`<div class="alert alert-success" role="alert">
      <h5 class="alert-heading">Statistically significant</h5>
      <p>The p-value is <strong>${p_value_clean}</strong> and our threshold for α is <strong>${alpha}</strong></p>
      <p>In a world where there is no relationship between x and y, the probability of seeing a slope of at least <strong>${obs_stat_abs}</strong> is <strong>${p_percent}</strong></p>
      <p>Since ${p_value_clean} is less than ${alpha}, we have enough evidence to say that the slope is <strong>statistically significant.</strong></p>
    </div>`;
  } else {
    return html`<div class="alert alert-warning" role="alert">
      <h5 class="alert-heading">Not statistically significant</h5>
      <p>The p-value is <strong>${p_value_clean}</strong> and our threshold for α is <strong>${alpha}</strong></p>
      <p>In a world where there is no relationship between x and y, the probability of seeing a slope of at least <strong>${obs_stat_abs}</strong> is <strong>${p_percent}</strong></p>
      <p>Since ${p_value_clean} is greater than ${alpha}, we don't have enough evidence to say that the slope doesn't come from the null world. The slope is thus <strong>not statistically significant.</strong></p>
      <hr>
      <p style="font-size: 0.9em;">This does <strong>not</strong> mean that there is no relationship between x and y! We just don't have enough evidence to judge if there's a relationship.</p>
    </div>`;
  }
}
```

::::

:::: {.g-col-12 .g-col-md-6}

```{ojs}
//| echo: false

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  x: { label: "Slope" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(null_bins, {
      x1: "x0",
      x2: "x1",
      y: "count",
      fill: d => d.extreme ? clrs.coral + "aa" : clrs.gray,
      stroke: "white",
      strokeWidth: 0.5
    }),
    Plot.ruleX([obs_stat], { stroke: "red", strokeWidth: 3 }),
    ...statLabel(obs_stat, d => `δ = ${d.toFixed(3)}`, 20,
      null_dist.map(d => d.stat)),
    ...statLabel(obs_stat, () => `p = ${p_value_clean}`, 45,
      null_dist.map(d => d.stat))
  ]
})

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  x: {
    label: "Explanatory variable (x)",
    domain: [0, 10]
  },
  y: { label: "Outcome variable (y)" },
  marks: [
    ...null_results.lines.map(l =>
      Plot.line(
        [
          { x: 0, y: l.intercept },
          { x: 10, y: l.intercept + l.slope * 10 }
        ],
        {
          x: "x",
          y: "y",
          stroke: "black",
          strokeWidth: 0.1,
          strokeOpacity: 0.3
        }
      )
    ),
    Plot.dot(sample_data, {
      x: "x",
      y: "y",
      fill: clrs.navy,
      fillOpacity: 0.7,
      r: 3
    }),
    Plot.line(
      [
        { x: 0, y: obs_fit.intercept },
        { x: 10, y: obs_fit.intercept + obs_fit.slope * 10 }
      ],
      {
        x: "x",
        y: "y",
        stroke: "red",
        strokeWidth: 3
      }
    )
  ]
})
```

::::

:::::

### Evidentiary standards

{{< include _evidentiary-standards.qmd >}}

::::::

:::::::

::::::::

:::::::::

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.height = 6 * 0.618,
  fig.align = "center",
  out.width = "80%"
)

options(easystats_display_format = "tt", width = 300)
```

## Flipper length and body mass

```{r}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(infer)
library(scales)
library(parameters)
library(tinytable)

penguins <- penguins |> drop_na(sex)

theme_set(theme_minimal(base_size = 14))

clrs <- c(
  "#f3d567",
  "#ee9b43",
  "#e74b47",
  "#b80422",
  "#172767",
  "#19798b"
)

set.seed(564058)

lbl_comma <- label_comma(style_negative = "minus")
lbl_p <- label_pvalue(prefix = c("< ", "", "> "))
lbl_p_pct <- \(x) ifelse(x < 0.001, "< 0.1%", label_percent(accuracy = 0.01)(x))
```

For this example, we want to know if flipper length predicts body mass in penguins near Palmer Station, Antarctica. Here's a scatterplot of the relationship:

```{r}
#| echo: false
#| message: false

ggplot(penguins, aes(x = flipper_len, y = body_mass)) +
  geom_point(color = clrs[5], size = 2) +
  geom_smooth(method = "lm", color = clrs[4], se = FALSE) +
  scale_y_continuous(labels = label_comma()) +
  labs(x = "Flipper length (mm)", y = "Body mass (g)")
```

There's a clear positive trend—penguins with longer flippers tend to be heavier. But is that relationship real, or could it just be due to random chance? Time for hypothesis testing!

First, we'll load some packages:

```{.r}
library(tidyverse)
library(infer)
library(parameters)

penguins <- penguins |> drop_na(sex)
```


## Null hypothesis inference with {infer}

::::: {.panel-tabset .nav-pills}

### Step 1: Calculate $\delta$

The sample statistic we're interested in is the regression slope—the estimated change in body mass for a one-unit (1 mm) increase in flipper length.

```{r}
#| warning: false
#| message: false

delta <- penguins |>
  specify(body_mass ~ flipper_len) |>
  calculate(stat = "slope")
delta
```

The slope is **`{r} round(delta$stat, 2)`**, meaning that for every additional millimeter of flipper length, body mass increases by about **`{r} round(delta$stat, 1)` grams.**


### Step 2: Simulate null world

We create a null distribution by shuffling (or "permuting") the flipper length values. This simulates a world where all the real, measured values of both flipper length and body mass are still the same, but where the relationship between them doesn't matter. **This eliminates any association between flipper length and body mass.**

```{r}
#| warning: false
#| message: false

shuffled_data <- penguins |>
  specify(body_mass ~ flipper_len) |>
  hypothesize(null = "independence") |>
  generate(reps = 5000, type = "permute")
```

Next we fit a regression in each of these 5,000 shuffled worlds and extract the slope:

```{r}
null_world <- shuffled_data |>
  calculate(stat = "slope")
null_world
```

Here's what this null world looks like:

```{r}
null_world |>
  visualize()
```

Notice that the slopes are centered around 0, reflecting a world where flipper length and body mass are unrelated.

### Step 3: Put $\delta$ in the null world

Next we put δ inside that null world to see how comfortably it fits there.

```{r}
null_world |>
  visualize() +
  shade_p_value(obs_stat = delta, direction = NULL)
```

That's *way* far to the right and doesn't look likely *at all*. A slope of `{r} round(delta$stat, 2)` is really unlikely in a world where flipper length and body mass are unrelated.

### Step 4: p-value

We can quantify the probability of seeing that red line in a null world. This is a **p-value**—the probability of seeing a slope at least that extreme in a world where there's no relationship between flipper length and body mass.

```{r}
null_world |>
  visualize() +
  shade_p_value(obs_stat = delta, direction = "two-sided")
```

```{r}
p_value <- null_world |>
  get_p_value(obs_stat = delta, direction = "two-sided")
p_value
```

The p-value is **`{r} lbl_p(p_value$p_value)`**. This means that in a world where flipper length has no relationship to body mass, there is a **`{r} lbl_p_pct(p_value$p_value)`** chance of seeing a slope at least as extreme as **`{r} round(delta$stat, 2)`**.

### Step 5: Decision

Finally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we *aren't* in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).

Using an α of 0.05, the p-value is **`{r} lbl_p(p_value$p_value)`**, which is less than 0.05. We have enough evidence to say that the relationship between flipper length and body mass is **statistically significant.**

```{r}
null_world |>
  visualize() +
  shade_p_value(obs_stat = delta, direction = "two-sided")
```

:::::


## Null hypothesis inference with `lm()`

In practice, most people do not simulate null worlds. Instead, they fit a regression model with `lm()`, which uses a *t*-distribution to approximate the null world mathematically and test whether each coefficient is different from 0. **The intuition is the same**: a p-value is still the probability of seeing a slope at least that extreme in a world where the true slope is 0.

```{r}
model <- lm(body_mass ~ flipper_len, data = penguins)
summary(model)
```

Buried in that output is the p-value for the `flipper_len` coefficient: p < 2.2e-16, or p < 2.2 × 10^−16^. That's really tiny. In a world where flipper length had no relationship with body mass, it would be virtually impossible to see a slope as extreme as `{r} round(delta$stat, 2)`. We have enough evidence to declare that the relationship is statistically significant.

If you don't like all that text output, you can feed the model to the `model_parameters()` function from [the {parameters} package](https://easystats.github.io/parameters/):

```{r}
model |>
  model_parameters() |>
  display(caption = "")
```
