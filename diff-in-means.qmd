---
title: "Difference in means"
---

::: {.column-screen .hero-banner-mini}

# Difference in means

◎◉○

:::

## Live simulation

::::::::: {.column-screen-inset}

```{ojs}
//| echo: false

jstat = require("jstat@1.9.6")

clrs = ({
  gold: "#f3d567",
  orange: "#ee9b43",
  coral: "#e74b47",
  crimson: "#b80422",
  navy: "#172767",
  teal: "#19798b",
  gray: "#4d4d4d"
})

function statLabel(value, textFn, dy, nullValues) {
  const extent = d3.extent([...nullValues, value]);
  const range = extent[1] - extent[0];
  const pos = range === 0 ? 0.5 :
    (value - extent[0]) / range;
  const textAnchor = pos > 0.82 ? "end" :
    pos < 0.18 ? "start" : "middle";
  const dx = textAnchor === "end" ? -10 :
    textAnchor === "start" ? 10 : 0;

  const common = {
    x: d => d, frameAnchor: "top", dy, dx,
    text: textFn,
    fontWeight: "bold", fontSize: 14,
    textAnchor, paintOrder: "stroke"
  };

  return [
    Plot.text([value], {
      ...common,
      stroke: "white", strokeWidth: 4, fill: "black"
    })
  ];
}
```

:::::::: {.grid}

::::::: {.g-col-12 .g-col-md-4 .sticky}

```{ojs}
//| echo: false

viewof sample_size = Inputs.range([30, 2000], {
  step: 10,
  value: 100,
  label: "Sample size:"
})

viewof effect_size = Inputs.range([-10, 10], {
  step: 1,
  value: 5,
  label: "Difference:"
})

viewof spread = Inputs.range([1, 30], {
  step: 1,
  value: 10,
  label: "Spread (σ):"
})
```

```{ojs}
//| echo: false

sample_data = {
  const data = [];
  for (let i = 0; i < sample_size; i++) {
    const group = Math.random() < 0.5 ? "A" : "B";
    const mean = group === "A" ? 50 : 50 + effect_size;
    data.push({
      group,
      outcome: jstat.normal.sample(mean, spread)
    });
  }
  return data;
}

Plot.plot({
  style: { fontSize: "13px" },
  marginLeft: 50,
  height: 280,
  x: { label: "Outcome" },
  y: { label: null },
  fy: { label: null, domain: ["A", "B"] },
  color: {
    domain: ["A", "B"],
    range: [clrs.navy, clrs.orange]
  },
  marks: [
    Plot.rectY(
      sample_data,
      Plot.binX(
        { y: "count" },
        {
          x: "outcome",
          fill: "group",
          fy: "group",
          thresholds: 20
        }
      )
    ),
    Plot.ruleX(
      [
        { value: group_stats.meanA, group: "A" },
        { value: group_stats.meanB, group: "B" }
      ],
      {
        x: "value",
        fy: "group",
        stroke: "black",
        strokeWidth: 2.5,
        strokeDasharray: "4,3"
      }
    ),
    Plot.text(
      [
        { value: group_stats.meanA, group: "A",
          label: group_stats.meanA.toFixed(1) },
        { value: group_stats.meanB, group: "B",
          label: group_stats.meanB.toFixed(1) }
      ],
      {
        x: "value",
        fy: "group",
        text: "label",
        frameAnchor: "top",
        dy: 12,
        fontSize: 14,
        fontWeight: "bold",
        fill: "black",
        stroke: "white",
        strokeWidth: 4,
        paintOrder: "stroke"
      }
    ),
    Plot.ruleY([0])
  ]
})
```

:::::::

::::::: {.g-col-12 .g-col-md-8}

:::::: {.panel-tabset .nav-pills .simulation}

## Step 1: Calculate δ

The sample statistic (δ) is the **difference in means** between the two groups.

```{ojs}
//| echo: false

group_stats = {
  const groups = { A: [], B: [] };
  for (const d of sample_data) {
    groups[d.group].push(d.outcome);
  }

  const meanA = d3.mean(groups.A);
  const meanB = d3.mean(groups.B);
  const nA = groups.A.length;
  const nB = groups.B.length;

  const seA = d3.deviation(groups.A) / Math.sqrt(nA);
  const seB = d3.deviation(groups.B) / Math.sqrt(nB);
  const ciA = [
    meanA - 1.96 * seA,
    meanA + 1.96 * seA
  ];
  const ciB = [
    meanB - 1.96 * seB,
    meanB + 1.96 * seB
  ];

  const diff = meanB - meanA;
  const seDiff = Math.sqrt(
    seA * seA + seB * seB
  );
  const ciDiff = [
    diff - 1.96 * seDiff,
    diff + 1.96 * seDiff
  ];

  return {
    meanA, meanB, nA, nB, diff,
    ciA, ciB, ciDiff
  };
}

obs_stat = group_stats.diff

obs_stat_abs = Math.abs(obs_stat).toFixed(2)
```

The difference in means is **`{ojs} obs_stat.toFixed(2)`**.

```{ojs}
//| echo: false

html`<table class="table table-sm" style="max-width: 500px;">
  <thead>
    <tr>
      <th>Group</th>
      <th>N</th>
      <th>Average</th>
      <th>95% CI</th>
    </tr>
  </thead>
  <tbody>
    <tr style="background-color: ${clrs.navy}11;">
      <td>A</td>
      <td>${group_stats.nA}</td>
      <td>${group_stats.meanA.toFixed(2)}</td>
      <td>[${group_stats.ciA[0].toFixed(2)}, ${group_stats.ciA[1].toFixed(2)}]</td>
    </tr>
    <tr style="background-color: ${clrs.orange}22;">
      <td>B</td>
      <td>${group_stats.nB}</td>
      <td>${group_stats.meanB.toFixed(2)}</td>
      <td>[${group_stats.ciB[0].toFixed(2)}, ${group_stats.ciB[1].toFixed(2)}]</td>
    </tr>
    <tr style="background-color: ${clrs.crimson}; color: white; font-weight: bold;">
      <td>Difference (B − A)</td>
      <td></td>
      <td>${group_stats.diff.toFixed(2)}</td>
      <td>[${group_stats.ciDiff[0].toFixed(2)}, ${group_stats.ciDiff[1].toFixed(2)}]</td>
    </tr>
  </tbody>
</table>`
```

## Step 2: Simulate null world

We create a null distribution by shuffling (or "permuting" to use the official stats term) the group labels. This simulates a world where all the real, measured values are still the same, but where group assignment doesn't matter. **This eliminates all differences between the groups.**

Think of this as being a world where there are no differences between the two groups. Importantly, this *doesn't* mean that the measured difference between the groups is *exactly* 0. There is variation in the data, and that variation is reflected in the null world. What it means is that in the null world, the difference between the two groups is 0 ± some amount.

Here's what one shuffle looks like. Notice that the outcome values stay the same—only the group labels get reassigned:

```{ojs}
//| echo: false

viewof reshuffle = Inputs.button("Reshuffle")
```

```{ojs}
//| echo: false

shuffle_preview = {
  reshuffle;
  const subset = sample_data.slice(0, 8);
  const groups = subset.map(d => d.group);
  const shuffled = groups.slice();
  for (let i = shuffled.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];
  }
  return {
    original: subset.map((d, i) => ({
      " ": i + 1,
      Group: d.group,
      Outcome: +d.outcome.toFixed(1)
    })),
    shuffled: subset.map((d, i) => ({
      " ": i + 1,
      Group: shuffled[i],
      Outcome: +d.outcome.toFixed(1)
    }))
  };
}
```

::::: {.grid .shuffled}

:::: {.g-col-12 .g-col-sm-6 .g-col-lg-5 .g-col-xl-4}

**Original data**

```{ojs}
//| echo: false

Inputs.table(shuffle_preview.original, {
  columns: [" ", "Group", "Outcome"],
  rows: 12,
  sort: false,
  select: false
})
```

::::

:::: {.g-col-12 .g-col-sm-6 .g-col-lg-5 .g-col-xl-4}

**Shuffled data**

```{ojs}
//| echo: false

Inputs.table(shuffle_preview.shuffled, {
  columns: [" ", "Group", "Outcome"],
  rows: 12,
  sort: false,
  select: false
})
```

::::

:::::

When we do this shuffle hundreds of times and compute the difference in means each time, we get a **null distribution**—a picture of what differences look like in a world where groups don't matter.

Here's what this null world looks like:

```{ojs}
//| echo: false

viewof n_reps = Inputs.range([100, 2000], {
  step: 100,
  value: 500,
  label: "Number of simulations:"
})
```

::: {style="width: 75%;"}

```{ojs}
//| echo: false

null_dist = {
  const outcomes = sample_data.map(d => d.outcome);
  const groups = sample_data.map(d => d.group);
  const n = outcomes.length;
  const results = [];

  for (let r = 0; r < n_reps; r++) {
    // Fisher-Yates shuffle of group labels
    const shuffled = groups.slice();
    for (let i = n - 1; i > 0; i--) {
      const j = Math.floor(Math.random() * (i + 1));
      [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];
    }

    let sumA = 0, nA = 0, sumB = 0, nB = 0;
    for (let i = 0; i < n; i++) {
      if (shuffled[i] === "B") {
        sumB += outcomes[i];
        nB++;
      } else {
        sumA += outcomes[i];
        nA++;
      }
    }
    results.push({ stat: sumB / nB - sumA / nA });
  }
  return results;
}

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: { label: "Difference (B − A)" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(
      null_dist,
      Plot.binX(
        { y: "count" },
        { x: "stat", fill: clrs.gray }
      )
    )
  ]
})
```

:::

## Step 3: Put δ in the null world

Next we put δ inside that null world and see how comfortably it fits there.

Is it surprising to see the red line in this null world? Is the line way out to one of the sides, or is it near the middle with the rest of the null world?

::: {style="width: 75%;"}

```{ojs}
//| echo: false

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: { label: "Difference (B − A)" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(
      null_dist,
      Plot.binX(
        { y: "count" },
        { x: "stat", fill: clrs.gray }
      )
    ),
    Plot.ruleX([obs_stat], { stroke: "red", strokeWidth: 3 }),
    ...statLabel(obs_stat, d => `δ = ${d.toFixed(2)}`, 20,
      null_dist.map(d => d.stat))
  ]
})
```

:::

## Step 4: p-value

We can actually quantify the probability of seeing that red line in a null world. This is a **p-value**—the probability of seeing a δ at least that extreme in a world where there's no difference between the group averages.

::: {style="width: 75%;"}

```{ojs}
//| echo: false

p_value = {
  const abs_obs = Math.abs(obs_stat);
  const extreme = null_dist.filter(
    d => Math.abs(d.stat) >= abs_obs
  ).length;
  return extreme / null_dist.length;
}

p_value_clean = p_value === 0
  ? "< 0.001"
  : p_value.toFixed(3)

p_percent = p_value === 0
  ? "< 0.1%"
  : (p_value * 100).toFixed(1) + "%"

// Pre-bin the null distribution so entire bins get colored
null_bins = {
  const values = null_dist.map(d => d.stat);
  const bin = d3.bin().thresholds(30);
  const bins = bin(values);
  const abs_obs = Math.abs(obs_stat);
  return bins.map(b => ({
    x0: b.x0,
    x1: b.x1,
    count: b.length,
    midpoint: (b.x0 + b.x1) / 2,
    extreme: Math.abs((b.x0 + b.x1) / 2) >= abs_obs
  }));
}

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  width: 500,
  x: { label: "Difference (B − A)" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(null_bins, {
      x1: "x0",
      x2: "x1",
      y: "count",
      fill: d => d.extreme ? clrs.coral + "aa" : clrs.gray,
      stroke: "white",
      strokeWidth: 0.5
    }),
    Plot.ruleX([obs_stat], { stroke: "red", strokeWidth: 3 }),
    ...statLabel(obs_stat, d => `δ = ${d.toFixed(2)}`, 20,
      null_dist.map(d => d.stat)),
    ...statLabel(obs_stat, () => `p = ${p_value_clean}`, 45,
      null_dist.map(d => d.stat))
  ]
})
```

The p-value is **`{ojs} p_value_clean`**

```{ojs}
//| echo: false

html`<p>This means that in a world where there is no difference between the groups, there is a <strong>${p_percent}</strong> chance of seeing a difference of at least <strong>${obs_stat_abs}</strong></p>`
```

:::

## Step 5: Decision

Finally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we *aren't* in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).

There are lots of possible thresholds. By convention, most people use a threshold (often shortened to $\alpha$) of 0.05, or 5%. But that's not required! You could have a lower standard with an $\alpha$ of 0.1 (10%), or a higher standard with an $\alpha$ of 0.01 (1%).

```{ojs}
//| echo: false

viewof alpha = Inputs.select([0.10, 0.05, 0.01], {
  label: "Significance threshold (α):",
  value: 0.05
})
```

::::: {.grid}

:::: {.g-col-12 .g-col-md-6}

```{ojs}
//| echo: false

{
  if (p_value < alpha) {
    return html`<div class="alert alert-success" role="alert">
      <h5 class="alert-heading">Statistically significant</h5>
      <p>The p-value is <strong>${p_value_clean}</strong> and our threshold for α is <strong>${alpha}</strong></p>
      <p>In a world where there is no difference between the groups, the probability of seeing a difference of at least <strong>${obs_stat_abs}</strong> is <strong>${p_percent}</strong></p>
      <p>Since ${p_value_clean} is less than ${alpha}, we have enough evidence to say that the difference is <strong>statistically significant.</strong></p>
    </div>`;
  } else {
    return html`<div class="alert alert-warning" role="alert">
      <h5 class="alert-heading">Not statistically significant</h5>
      <p>The p-value is <strong>${p_value_clean}</strong> and our threshold for α is <strong>${alpha}</strong></p>
      <p>In a world where there is no difference between the groups, the probability of seeing a difference of at least <strong>${obs_stat_abs}</strong> is <strong>${p_percent}</strong></p>
      <p>Since ${p_value_clean} is greater than ${alpha}, we don't have enough evidence to say that the difference doesn't come from the null world. The difference is thus <strong>not statistically significant.</strong></p>
      <hr>
      <p style="font-size: 0.9em;">This does <strong>not</strong> mean that there is no difference between the groups! We just don't have enough evidence to judge if there's a difference.</p>
    </div>`;
  }
}
```

::::

:::: {.g-col-12 .g-col-md-6}

```{ojs}
//| echo: false

Plot.plot({
  style: { fontSize: "13px" },
  height: 300,
  x: { label: "Difference (B − A)" },
  y: { label: "Count" },
  marks: [
    Plot.rectY(null_bins, {
      x1: "x0",
      x2: "x1",
      y: "count",
      fill: d => d.extreme ? clrs.coral + "aa" : clrs.gray,
      stroke: "white",
      strokeWidth: 0.5
    }),
    Plot.ruleX([obs_stat], { stroke: "red", strokeWidth: 3 }),
    ...statLabel(obs_stat, d => `δ = ${d.toFixed(2)}`, 20,
      null_dist.map(d => d.stat)),
    ...statLabel(obs_stat, () => `p = ${p_value_clean}`, 45,
      null_dist.map(d => d.stat))
  ]
})
```

::::

:::::

### Evidentiary standards

{{< include _evidentiary-standards.qmd >}}

::::::

:::::::

::::::::

:::::::::

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6, 
  fig.height = 6 * 0.618, 
  fig.align = "center", 
  out.width = "80%"
)

# Make the display() function create {tinytable} tables
options(easystats_display_format = "tt", width = 300)
```

## Differences in penguin weights

For this example, we'll look at the body mass of penguins near Palmer Station, Antarctica. At first glance, it looks like there are some definite species-based differences in average penguin weight:

```{r}
#| echo: false 
#| warning: false
#| message: false

library(tidyverse)
library(infer)
library(scales)
library(ggdist)
library(ggbeeswarm)
library(parameters)
library(tinytable)

penguins <- penguins |> drop_na(sex)

theme_set(theme_minimal(base_size = 14))

# From MoMAColors::moma.colors("OKeeffe")
clrs <- c(
  "#f3d567",
  "#ee9b43",
  "#e74b47",
  "#b80422",
  "#172767",
  "#19798b"
)

set.seed(564058)  # Seed from Random.org

lbl_comma <- label_comma(style_negative = "minus")
lbl_p <- label_pvalue(prefix = c("< ", "", "> "))
lbl_p_pct <- \(x) ifelse(x < 0.001, "< 0.1%", label_percent(accuracy = 0.01)(x))

penguins_chin_gen <- penguins |>
  filter(species %in% c("Chinstrap", "Gentoo"))
```

```{r}
#| echo: false

avg_weight <- penguins |>
  group_by(species) |>
  summarize(avg_weight = mean(body_mass))

avg_weight |>
  setNames(c("Species", "Average weight (g)")) |>
  tt() |> 
  format_tt(digits = 2) |> 
  style_tt(align = "lc")
```

```{r}
#| echo: false

ggplot(penguins, aes(x = species, y = body_mass, color = species)) +
  geom_beeswarm(side = -1, size = 1, cex = 1.5) +
  stat_pointinterval(.width = 0.95, position = position_nudge(x = 0.2)) +
  scale_y_continuous(labels = label_number(scale_cut = cut_si("g"))) +
  scale_color_manual(values = c(clrs[2], clrs[4], clrs[5]), guide = "none") +
  labs(x = "Species", y = "Body mass")
```

Let's look specifically at the difference in the average body mass for just Chinstrap and Gentoo penguins. First, we'll load some packages:

```{.r}
library(tidyverse)
library(infer)
library(parameters)

penguins <- penguins |> drop_na(sex)
```

With some filtering, grouping, and summarizing, we can find the difference in means and see that Chinstraps seem to be a lot lighter than Gentoos, on average:

```{r}
penguins |>
  filter(species %in% c("Chinstrap", "Gentoo")) |>
  group_by(species) |>
  summarize(avg_weight = mean(body_mass)) |>
  mutate(difference = c(NA, avg_weight[1] - avg_weight[2]))
```

But is that difference real? Could it potentially be zero? We need to do some hypothesis testing.


## Null hypothesis inference with {infer}

::::: {.panel-tabset .nav-pills}

### Step 1: Calculate $\delta$

The sample statistic we're interested in is the difference in means between Chinstrap and Gentoo penguins.

To find this, we'll filter the data to only keep those two species, use `specify()` to define the response and explanatory variables we're using, and use `calculate()` to calculate the difference in means:

```{r}
#| warning: false
#| message: false

delta <- penguins |>
  filter(species %in% c("Chinstrap", "Gentoo")) |>
  specify(body_mass ~ species) |>
  calculate(stat = "diff in means", order = c("Chinstrap", "Gentoo"))
delta
```

The difference in means between Chinstraps and Gentoos is **`{r} lbl_comma(delta$stat)` grams.**


### Step 2: Simulate null world

We create a null distribution by shuffling (or "permuting" to use the official stats term) the species label. This simulates a world where all the penguin weights are still the same, but where species assignment doesn't matter. **This eliminates all differences between the species.**

```{r}
#| warning: false
#| message: false

shuffled_data <- penguins |>
  filter(species %in% c("Chinstrap", "Gentoo")) |>
  specify(body_mass ~ species) |>
  hypothesize(null = "independence") |>
  generate(reps = 5000, type = "permute")
shuffled_data
```

The resulting data frame has `{r} lbl_comma(nrow(shuffled_data))` rows! That's because we made 5,000 versions of the original data for `{r} lbl_comma(nrow(penguins_chin_gen))` Chinstrap and Gentoo penguins.

Next we need to calculate the $\delta$ in each of these 5,000 worlds:

```{r}
null_world <- shuffled_data |>
  calculate(stat = "diff in means", order = c("Chinstrap", "Gentoo"))
null_world
```

The distribution of all these differences in means across the different simulated datasets creates a **null world**, or null distribution. Here's what this null world looks like:

```{r}
null_world |>
  visualize()
```

Importantly, notice that the difference between species is not *exactly* 0. There is variation in the actual data, and that variation is reflected in the null world. In a world where there is no difference between the species, the difference is 0 ± some amount. In this case, it's 0 ± ≈250ish grams.

### Step 3: Put $\delta$ in the null world

Next we put $\delta$ inside that null world to see how comfortably it fits there.

```{r}
null_world |>
  visualize() +
  shade_p_value(obs_stat = delta, direction = NULL)
```

That's *way* far to the right and doesn't look likely *at all*. A difference of `{r} lbl_comma(delta$stat)` g is really unlikely in a world where there's no difference between the species.

### Step 4: p-value

We can actually quantify the probability of seeing that red line in a null world. This is a **p-value**—the probability of seeing a $\delta$ at least that extreme in a world where there's no difference between the species averages.

```{r}
null_world |>
  visualize() +
  shade_p_value(obs_stat = delta, direction = "two-sided")
```

```{r}
p_value <- null_world |>
  get_p_value(obs_stat = delta, direction = "two-sided")
p_value
```

The p-value is **`{r} lbl_p(p_value$p_value)`**. (It's so tiny that `get_p_value()` reports it as 0, along with an ominous warning. Technically it's not zero—it's just really really tiny.)

This means that in a world where there is no difference between the groups, there is a **< 0.1%** chance of seeing a difference of at least **`{r} lbl_comma(delta$stat)`**.

### Step 5: Decision

Finally, we have to decide if the p-value meets an evidentiary standard or threshold that would provide us with enough evidence that we *aren't* in the null world (or, in more statsy terms, enough evidence to reject the null hypothesis).

There are lots of possible thresholds. By convention, most people use a threshold (often shortened to $\alpha$) of 0.05, or 5%. But that's not required! You could have a lower standard with an $\alpha$ of 0.1 (10%), or a higher standard with an $\alpha$ of 0.01 (1%). In this case, we'll assume an $\alpha$ of 0.05.

In this case, the p-value is **`{r} lbl_p(p_value$p_value)`** and our threshold for $\alpha$ is **0.05**.

In a world where there is no difference between the two species, the probability of seeing a difference as extreme as **`{r} lbl_comma(delta$stat)`** is **`{r} lbl_p_pct(p_value$p_value)`**.

Since `{r} lbl_p(p_value$p_value)` is less than 0.05, we have enough evidence to say that the difference is **statistically significant.**

```{r}
null_world |>
  visualize() +
  shade_p_value(obs_stat = delta, direction = "two-sided")
```

:::::


## Null hypothesis inference with `t.test()`

In practice, most people do not simulate null worlds. Instead, they'll use a built-in test that uses a known theoretical distribution of what the null world is assumed to look like (like the *t*, *F*, and $\chi^2$ distributions), and calculate a p-value based on that null distribution. This theoretical, mathematical p-value is what you see in regular statistical output.

Even though these values are not based on simulations, **the intuition is the same**: a p-value is still the probability of seeing a $\delta$ at least that extreme in a world where there is no difference.

To find the difference in group means, we can conduct a *t*-test with `t.test()`:

```{r}
t.test(
  body_mass ~ species,
  data = filter(penguins, species %in% c("Chinstrap", "Gentoo"))
)
```

Buried in that giant wall of text is the p-value: p < 2.2e-16, or p < 2.2 × 10^−16^. That's really tiny. In a world where Chinstrap and Gentoo penguins had the same average body mass, it would be virtually impossible to see a difference as extreme as `{r} lbl_comma(delta$stat)`. We have enough evidence to declare that difference is statistically significant.

If you don't like all that text output, you can feed the results of `t.test()` to the `model_parameters()` function from [the {parameters} package](https://easystats.github.io/parameters/). This creates a nice little table with the different group means, the difference, the confidence interval, and the p-value. Feed *that* into `display()` and you'll get a nicely rendered table.

```{r}
t.test(
  body_mass ~ species,
  data = filter(penguins, species %in% c("Chinstrap", "Gentoo"))
) |>
  model_parameters() |>
  display(caption = "")
```

::: {.callout-warning}
### Side caveat: Different "flavors" of tests

```{r}
#| include: false

t_result <- t.test(
  body_mass ~ species,
  data = filter(penguins, species %in% c("Chinstrap", "Gentoo"))
)

df_val <- t_result$parameter
t_stat <- t_result$statistic
```

Since we're not simulating the null world, we're making strong assumptions about what the null world looks like. In the case of this *t*-test, we're assuming it follows a *t*-distribution with `{r} round(df_val, 1)` degrees of freedom. That looks like this:

```{r}
#| echo: false
#| out-width: 60%

ggplot() +
  stat_function(
    geom = "area",
    fun = \(x) dt(x, df = df_val),
    xlim = c(-5, 5),
    fill = "grey30"
  ) +
  labs(
    x = "t-statistic",
    y = "Density",
    title = str_glue(
      "Null world based on a t-distribution (df ≈ {round(df_val, 1)})"
    )
  )
```

We can then put a standardized version of our observed $\delta$—the *t* value of `{r} round(t_stat, 1)`—in that mathematical null world and find the exact probability of seeing it there:

```{r}
#| echo: false
#| out-width: 60%

ggplot() +
  stat_function(
    geom = "area",
    fun = \(x) dt(x, df = df_val),
    xlim = c(-5, 5),
    fill = "grey30"
  ) +
  geom_vline(xintercept = t_stat, color = "red", linewidth = 1) +
  labs(
    x = "t-statistic",
    y = "Density",
    title = str_glue(
      "Null world based on a t-distribution (df ≈ {round(df_val, 1)})"
    )
  )
```

We can only assume that this precise *t*-distribution represents a null world under certain conditions. This is why there are [so many statistical test flowcharts](https://www.google.com/search?q=statistical+test+flow+chart)—you have to make sure you choose the test that matches the conditions and characteristics of your data.

For example, different versions of the *t*-test make different assumptions about whether the two groups have equal variances. By default, R uses Welch's *t*-test, which is designed for group means with unequal variances, while the Student *t*-test assumes that group means have equal variances. Technically, in order to check which flavor of *t*-test we need to run, we'd need to test for equality of variances, which involves a separate statistical test with its own null hypothesis. If the variances are equal, we'd use `t.test(..., var.equal = TRUE)`; if the variances are unequal, we'd use `t.test(..., var.equal = FALSE)`. The only way to remember all these assumption checks and all the different versions of statistical tests is to consult a flowchart. It can be miserable.

**If you simulate, you can skip all that.** Your null world is based on the qualities of your observed data, not an idealized mathematical distribution.
:::
